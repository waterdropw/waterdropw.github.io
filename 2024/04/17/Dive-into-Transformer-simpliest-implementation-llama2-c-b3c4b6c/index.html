<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0,viewport-fit=cover"><title>极简`Transformer`实现(基于`llama2.c`commit-b3c4b6c 24'2/13) | xbw's blog</title><meta name="author" content="xbw"><meta name="copyright" content="xbw"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="与早期版本（详情可以参考这里：4e23ad8 23&#39;7&#x2F;27）不同，目前的版本有两种模式，一是generate， 根据给定的prompt生成一个简短的英文小故事。二是chat， 需要输入system prompt和问题，生成一个回答并退出，没有多轮对话，并且回答与问题没有太大关联性而且有不少问题（尤其是15M模型），所以该模式还不完善，并且有一个 feature&#x2F;cha">
<meta property="og:type" content="article">
<meta property="og:title" content="极简&#96;Transformer&#96;实现(基于&#96;llama2.c&#96;commit-b3c4b6c 24&#39;2&#x2F;13)">
<meta property="og:url" content="https://waterdropw.github.io/2024/04/17/Dive-into-Transformer-simpliest-implementation-llama2-c-b3c4b6c/index.html">
<meta property="og:site_name" content="xbw&#39;s blog">
<meta property="og:description" content="与早期版本（详情可以参考这里：4e23ad8 23&#39;7&#x2F;27）不同，目前的版本有两种模式，一是generate， 根据给定的prompt生成一个简短的英文小故事。二是chat， 需要输入system prompt和问题，生成一个回答并退出，没有多轮对话，并且回答与问题没有太大关联性而且有不少问题（尤其是15M模型），所以该模式还不完善，并且有一个 feature&#x2F;cha">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://waterdropw.github.io/images/waterdrop.jpeg">
<meta property="article:published_time" content="2024-04-17T02:34:26.000Z">
<meta property="article:modified_time" content="2025-11-20T11:50:20.178Z">
<meta property="article:author" content="xbw">
<meta property="article:tag" content="AI">
<meta property="article:tag" content="transformer">
<meta property="article:tag" content="self-attention">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://waterdropw.github.io/images/waterdrop.jpeg"><link rel="shortcut icon" href="/img/favicon.png"><link rel="canonical" href="https://waterdropw.github.io/2024/04/17/Dive-into-Transformer-simpliest-implementation-llama2-c-b3c4b6c/index.html"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><meta name="google-site-verification" content="lXlWKu0XEea_JgmAK9kbcSzDAlGOvw3UBxKPUp03pz8"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox/fancybox.min.css" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = {
  root: '/',
  algolia: undefined,
  localSearch: undefined,
  translate: undefined,
  noticeOutdate: undefined,
  highlight: {"plugin":"highlighjs","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '',
  dateSuffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'fancybox',
  Snackbar: undefined,
  source: {
    justifiedGallery: {
      js: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery/dist/fjGallery.min.js',
      css: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery/dist/fjGallery.min.css'
    }
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isAnchor: false,
  percent: {
    toc: true,
    rightside: false,
  },
  autoDarkmode: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: '极简`Transformer`实现(基于`llama2.c`commit-b3c4b6c 24\'2/13)',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2025-11-20 19:50:20'
}</script><noscript><style type="text/css">
  #nav {
    opacity: 1
  }
  .justified-gallery img {
    opacity: 1
  }

  #recent-posts time,
  #post-meta time {
    display: inline !important
  }
</style></noscript><script>(win=>{
    win.saveToLocal = {
      set: function setWithExpiry(key, value, ttl) {
        if (ttl === 0) return
        const now = new Date()
        const expiryDay = ttl * 86400000
        const item = {
          value: value,
          expiry: now.getTime() + expiryDay,
        }
        localStorage.setItem(key, JSON.stringify(item))
      },

      get: function getWithExpiry(key) {
        const itemStr = localStorage.getItem(key)

        if (!itemStr) {
          return undefined
        }
        const item = JSON.parse(itemStr)
        const now = new Date()

        if (now.getTime() > item.expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return item.value
      }
    }
  
    win.getScript = url => new Promise((resolve, reject) => {
      const script = document.createElement('script')
      script.src = url
      script.async = true
      script.onerror = reject
      script.onload = script.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        script.onload = script.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(script)
    })
  
    win.getCSS = (url,id = false) => new Promise((resolve, reject) => {
      const link = document.createElement('link')
      link.rel = 'stylesheet'
      link.href = url
      if (id) link.id = id
      link.onerror = reject
      link.onload = link.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        link.onload = link.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(link)
    })
  
      win.activateDarkMode = function () {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = function () {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
          if (t === 'dark') activateDarkMode()
          else if (t === 'light') activateLightMode()
        
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
    const detectApple = () => {
      if(/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
    })(window)</script><!-- hexo injector head_end start -->
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css">

<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/hexo-math@4.0.0/dist/style.css">
<!-- hexo injector head_end end --><meta name="generator" content="Hexo 6.3.0"><link rel="alternate" href="/atom.xml" title="xbw's blog" type="application/atom+xml">
</head><body><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src="/images/waterdrop.jpeg" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="sidebar-site-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">12</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">15</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">15</div></a></div><hr class="custom-hr"/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 归档</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 目录</span></a></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fas fa-list"></i><span> 娱乐</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/music/"><i class="fa-fw fas fa-music"></i><span> 音乐</span></a></li><li><a class="site-page child" href="/movies/"><i class="fa-fw fas fa-video"></i><span> 电影</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> 链接</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url('/images/jellyfish.jpg')"><nav id="nav"><span id="blog-info"><a href="/" title="xbw's blog"><span class="site-name">xbw's blog</span></a></span><div id="menus"><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 归档</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 目录</span></a></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fas fa-list"></i><span> 娱乐</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/music/"><i class="fa-fw fas fa-music"></i><span> 音乐</span></a></li><li><a class="site-page child" href="/movies/"><i class="fa-fw fas fa-video"></i><span> 电影</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> 链接</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于</span></a></div></div><div id="toggle-menu"><a class="site-page" href="javascript:void(0);"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">极简`Transformer`实现(基于`llama2.c`commit-b3c4b6c 24'2/13)</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2024-04-17T02:34:26.000Z" title="发表于 2024-04-17 10:34:26">2024-04-17</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2025-11-20T11:50:20.178Z" title="更新于 2025-11-20 19:50:20">2025-11-20</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/AI/">AI</a><i class="fas fa-angle-right post-meta-separator"></i><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/AI/transformer/">transformer</a><i class="fas fa-angle-right post-meta-separator"></i><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/AI/transformer/self-attention/">self-attention</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title="极简`Transformer`实现(基于`llama2.c`commit-b3c4b6c 24'2/13)"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">阅读量:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><p>与早期版本（详情可以参考这里：<a href="/2024/04/17/Dive-into-Transformer-simpliest-implementation-llama2-c-4e23ad8/" title="极简&#96;Transformer&#96;实现(基于&#96;llama2.c&#96;commit-4e23ad 23&#39;7&#x2F;27)">4e23ad8 23&#39;7&#x2F;27</a>）不同，目前的版本有两种模式，一是<strong>generate</strong>， 根据给定的prompt生成一个简短的英文小故事。二是<strong>chat</strong>， 需要输入system prompt和问题，生成一个回答并退出，没有多轮对话，并且回答与问题没有太大关联性而且有不少问题（尤其是15M模型），所以该模式还不完善，并且有一个 <a href="https://github.com/karpathy/llama2.c/tree/feature/chat" title="feature&#x2F;chat" target="">feature&#x2F;chat</a> 分支处于开发中有兴趣可以关注该分支。我们主要关注代码更新的部分。</p>
<h1 id="执行流程"><a href="#执行流程" class="headerlink" title="执行流程"></a>执行流程</h1><p>该版本代码进行了重构，sampler封装到结构体里，其次把分阶段的执行流程各自封装到函数里面，调用逻辑会更清晰一些。同样的，<code>main</code>主要处理命令行参数，然后是模型推理流程。</p>
<h2 id="main-执行流程"><a href="#main-执行流程" class="headerlink" title="main 执行流程"></a><code>main</code> 执行流程</h2><ol>
<li>解析命令行参数，包括 checkpoint 文件路径, temperature, steps等。</li>
<li>调用<code>build_transformer</code>，会调用<code>read_checkpoint</code>加载checkpoint 文件内容，包括模型配置和权重。其次调用<code>malloc_run_state</code>初始化运行时变量，包括kv cache等。注意，这里没有再对<code>s-&gt;k</code>, <code>s-&gt;v</code>分配内存，直接指向kv cache相应的位置，省掉一次内存拷贝操作。</li>
<li>调用<code>build_tokenizer</code>，加载<code>tokenizer.bin</code>。</li>
<li>调用<code>build_sampler</code>，初始化采样对象。</li>
<li>接下来就是根据给定运行模式，调用<code>generate</code>或者<code>chat</code>执行生成或问答流程。</li>
<li>内存清理并退出</li>
</ol>
<h2 id="generate"><a href="#generate" class="headerlink" title="generate"></a><code>generate</code></h2><p>计算过程：</p>
<ol>
<li>首先调用<code>encode</code>，把用户输入的<strong>prompt</strong>编码为token，存储到<code>prompt_tokens</code>，维度为<code>(strlen(prompt)+3,)</code>的int数组。</li>
<li>然后从<code>prompt_tokens</code>的第一个token开始（通常是BOS）执行主循环生成文本，即根据给定的**seq_len(steps)**执行循环 <code>[0, steps]</code>：<ol>
<li>调用<code>forward</code>得到当前位置<code>pos</code>的<code>logits</code><ol>
<li>如果当前词元还是prompt，则从prompt里面取下一个token</li>
<li>否则，根据logits调用<code>sample</code>采样得到下一个token</li>
</ol>
</li>
<li>如果下一个token是BOS，则终止循环。</li>
<li>调用<code>decode</code>从tokenizer里面解码得到token对应的单词，并打印。</li>
<li>继续下一个循环</li>
</ol>
</li>
<li>生成完毕，释放<code>prompt_tokens</code>临时堆内存</li>
</ol>
<h2 id="chat"><a href="#chat" class="headerlink" title="chat"></a><code>chat</code></h2><p>Todo：</p>
<h2 id="forward"><a href="#forward" class="headerlink" title="forward"></a><code>forward</code></h2><p>模型推理的计算都封装到该函数里了，具体计算过程与<a href="/2024/04/17/Dive-into-Transformer-simpliest-implementation-llama2-c-4e23ad8/" title="极简&#96;Transformer&#96;实现(基于&#96;llama2.c&#96;commit-4e23ad 23&#39;7&#x2F;27)">4e23ad8 23&#39;7&#x2F;27</a>里面的<code>transformer</code>基本类似，差异是增加了对**Multi-Query Attention(MQA)<strong>和</strong>Grouped-Query Attention(GQA)**的支持。<br>先看下<code>MHA</code>、<code>MQA</code>及<code>GQA</code>之间的区别，</p>
<img src="/images/llama2/mha_mqa_gqa.jpg" class="" width="600" height="408" title="MHA&#x2F;MQA&#x2F;GQA">

<p>在该代码实现里，</p>
<ol>
<li>如果<code>n_kv_heads==n_heads</code>则为MHA，每个query都对应一组独立的key和value。</li>
<li>如果<code>n_kv_heads==1</code>则为MQA，所有query对应一组key和value。</li>
<li>否则<code>1&lt;n_kv_heads&lt;n_heads</code>则为GQA，多个query对应一组key和value，并且不止一组。</li>
</ol>
<p>具体的代码实现流程如下：</p>
<ol>
<li>初始化局部变量，新增<code>kv_dim = dim/n_heads * n_kv_heads = head_dim * n_kv_heads</code>，因为要跟query保持一致，所以kv head的<code>head_dim</code>也是<code>head_dim = dim/n_heads</code>固定的与<code>n_kv_heads</code>无关。补充一下，这里<code>kv_dim</code>代表每个token对应在kv cache里面的偏移大小或者说是每个token的dim维度，具体可以参考后面<strong>kv_cache内存结构</strong>。接下来进入正题，从<code>token_embedding_table</code>中取出当前token的embedding<code>(dim, )</code> 放入 <code>s-&gt;x</code> 中作为模型输入。</li>
<li>循环遍历每一层<code>for l in 0..n_layers</code>：<ol>
<li>对<code>s-&gt;x</code>计算<code>rmsnorm</code>并存入 <code>s-&gt;xb</code>；计算该层kv cache偏移，<code>loff=l*seq_len*kv_dim</code>，如上分析，kv_dim就是每个token的维度。</li>
<li>计算 Q、K、V，结果直接存入 <code>s-&gt;q</code>, <code>s-&gt;k</code>, <code>s-&gt;v</code>，而后两者是指向kv cache当前位置的，所以省掉一次内存拷贝。</li>
<li>计算<code>RoPE</code>编码的Q、K，结果存入 <code>s-&gt;q</code>, <code>s-&gt;k</code>。注意这里的改进，多头合在一起一并计算，效率更高。</li>
<li>计算多头注意力，循环遍历每个头<code>for h in 0..n_heads</code>:<ol>
<li>Q起始位置，Q原始维度<code>(dim, )</code>拆分多头后为<code>(n_heads, head_dim)</code>，所以第<code>h</code>头的起始位置为**<code>float* q = s-&gt;q + h * head_size</code></li>
<li>atten score <code>(n_heads, seq_len)</code>，因为是按头存放的，所以在第<code>h</code>头的起始位置就是 <code>s-&gt;att + h * p-&gt;seq_len</code></li>
<li>kv cache在<code>dim</code>维度拆分多头，并考虑<code>MQA/GQA</code>后维度为<code>(layer, seq_len, n_kv_heads, head_dim)</code>或者简化为<code>(layer, seq_len, kv_dim)</code>，因为<code>n_kv_heads &lt;= n_heads</code>要均分共享，这里的关键是需要计算query的第h个head对应的kv cache偏移在哪里。代码实现里面有个小技巧，我们先来看sequence维度，<code>t</code>位置的偏移为<code>t * kv_dim</code>，这个好理解，前面已经讲过，然后再计算<code>kv_dim</code>里面的偏移是多少，这里会用到临时变量<code>kv_mul = n_kv_heads / n_heads</code>, 则query第<code>h</code>头在第<code>t</code>个token维度内也就是<code>kv_dim</code>内，偏移为<code>(h/kv_mul) * head_size</code>。</li>
<li>计算该词元对<strong>它之前所有</strong>词元的注意力分数（包含自己），其实就是masked attention，<code>for t in 0..pos+1</code>:<ul>
<li>第 <code>t</code> 个词元对应在第 <code>h</code> 头内的偏移为 <code>t * kv_dim</code>，这个好理解，前面已经讲过。综上，<code>t</code>位置的cache为 <code>s-&gt;key_cache + loff + (h/kv_mul) * head_size + t * kv_dim</code>。</li>
<li>计算 $att(t)&#x3D;\sum_{i&#x3D;0}^{head\_dim}Q_i*K_i&#x2F;\sqrt{head\_dim}$ ，其维度为 <code>(seq_len, )</code>，但因为仅仅<code>[0, pos]</code>的值为有效的，也即每个已生成的词元都有一个分数。（注意，att数组内容在计算下一个token时被重新填充）</li>
</ul>
</li>
<li>att经过softmax转换为注意力权重 $att&#x3D;softmax(att)$</li>
<li>接下来一步，是把每个词元的<code>V</code>向量与其对应的<code>att</code>值相乘，然后加总形成一个新的<code>V</code>向量，存储到<code>s-&gt;xb</code>中，计算公式为 $\hat{V}&#x3D;\sum_{t&#x3D;0}^{pos}att(t)*V(t)$，其维度为<code>(head_dim,)</code>与<code>V</code>相同，所以head计算完成之后，维度为<code>(dim,)</code>至此，多头循环结束。</li>
</ol>
</li>
<li>注意力最后一步，与 $W^O$ 矩阵相乘，得到该层最终的注意力输出向量 (dim, )，存储到 <code>s-&gt;xb2</code></li>
<li>接下来是 residual 的 <code>Add&amp;Norm</code>，elemwise add 和 rmsnorm，结果存入 <code>s-&gt;xb</code></li>
<li>接下来是FFN网络计算: <code>self.w2(F.silu(self.w1(x)) * self.w3(x))</code><ol>
<li><code>s-&gt;hb=matmul(w-&gt;w1, s-&gt;xb)</code></li>
<li><code>s-&gt;hb2=matmul(w-&gt;w3, s-&gt;xb)</code></li>
<li><code>silu(x)=x*σ(x)</code> for <code>s-&gt;hb</code></li>
<li>elemwise mul <code>s-&gt;hb * s-&gt;hb2</code></li>
<li>matmul，得到FFN输出，存储在 <code>s-&gt;xb</code></li>
</ol>
</li>
<li>layer计算最后一步，residual 的 <code>Add&amp;Norm</code>，注意该实现中rmsnorm放到了layer循环的最开始，这样除最后一层外都会residual add之后执行rmsnorm，而最后一层则放在classifier 的rmsnorm。至此，layer循环结束</li>
</ol>
</li>
<li>Classifier 前面的 final rmsnorm， 就是针对最后一层residual的结果做rmsnorm，存入 <code>s-&gt;x</code>b 中</li>
<li>classifier into logits ，是个Linear层，从(dim,) → (vocab_size,) 的转换， 就是一个matmul: (vacob_size, dim) * (dim,1) → (vocab_size, 1) 结果就是logits，对应字典里每个token的分数。</li>
</ol>
<h1 id="其它函数"><a href="#其它函数" class="headerlink" title="其它函数"></a>其它函数</h1><h2 id="build-transformer"><a href="#build-transformer" class="headerlink" title="build_transformer"></a><code>build_transformer</code></h2><p>做了两件事情：</p>
<ol>
<li>读取<code>checkpoint</code>文件并解析<strong>模型配置参数</strong>和<strong>权重参数</strong>，存入结构体变量<code>t</code>里。<code>checkpoint</code>文件结构详见上一篇文章。</li>
<li>初始化运行时的<code>RunState</code>结构体，分配内存空间，包括模型输入输出，attention分数，kv cache等。<blockquote>
<p>注：<code>t-&gt;s-&gt;k</code>和<code>t-&gt;s-&gt;v</code>不分配内存空间，计算是直接读取kv cache对应的内存区域。这个跟之前的版本有区别。</p>
</blockquote>
</li>
</ol>
<figure class="highlight c"><figcaption><span>build_transformer</span><a target="_blank" rel="noopener" href="https://github.com/karpathy/llama2.c/blob/b3c4b6c3c4bbff42e5211293280307019368ccb5/run.c#L164">run.c</a></figcaption><table><tr><td class="gutter"><pre><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br></pre></td><td class="code"><pre><span class="line marked"><span class="type">void</span> <span class="title function_">build_transformer</span><span class="params">(Transformer *t, <span class="type">char</span>* checkpoint_path)</span> &#123;</span><br><span class="line">    <span class="comment">// read in the Config and the Weights from the checkpoint</span></span><br><span class="line">    read_checkpoint(checkpoint_path, &amp;t-&gt;config, &amp;t-&gt;weights, &amp;t-&gt;fd, &amp;t-&gt;data, &amp;t-&gt;file_size);</span><br><span class="line">    <span class="comment">// allocate the RunState buffers</span></span><br><span class="line">    malloc_run_state(&amp;t-&gt;state, &amp;t-&gt;config);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h2 id="build-tokenizer"><a href="#build-tokenizer" class="headerlink" title="build_tokenizer"></a><code>build_tokenizer</code></h2><p>tokenizer文件结构详见后面章节，注意跟之前的版本有差别且不兼容。此外新增了</p>
<ul>
<li><code>vocab_scores</code>维度<code>(vocab_size,)</code></li>
<li><code>byte_pieces</code>维度<code>(512,)</code>，初始化为 0~255 字符每个都带’\0’结尾隔开，总共512个字符。</li>
</ul>
<p>该函数代码基本上就是文件和内存操作，初始化<code>Tokenizer</code>结构体，具体来说就是把<code>tokenizer.bin</code>文件读取到内存中，总共<code>32k</code>个词元字符串，每个词元开辟一块字符串数组内存区域单独存储，所有字符串指针放到一个数组里，可以按顺序遍历，也就是给定token，作为index即可取出对应词元的字符串。</p>
<blockquote>
<p>文件结构详细信息可参考上一篇文章。</p>
</blockquote>
<figure class="highlight c"><figcaption><span>Tokenizer</span><a target="_blank" rel="noopener" href="https://github.com/karpathy/llama2.c/blob/b3c4b6c3c4bbff42e5211293280307019368ccb5/run.c#L364">run.c</a></figcaption><table><tr><td class="gutter"><pre><span class="line">364</span><br><span class="line">365</span><br><span class="line">366</span><br><span class="line">367</span><br><span class="line">368</span><br><span class="line">369</span><br><span class="line">370</span><br><span class="line">371</span><br><span class="line">372</span><br><span class="line">373</span><br><span class="line">374</span><br><span class="line">375</span><br><span class="line">376</span><br><span class="line">377</span><br><span class="line">378</span><br><span class="line">379</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// ----------------------------------------------------------------------------</span></span><br><span class="line"><span class="comment">// The Byte Pair Encoding (BPE) Tokenizer that translates strings &lt;-&gt; tokens</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">typedef</span> <span class="class"><span class="keyword">struct</span> &#123;</span></span><br><span class="line">    <span class="type">char</span> *str;</span><br><span class="line">    <span class="type">int</span> id;</span><br><span class="line marked">&#125; TokenIndex;</span><br><span class="line"></span><br><span class="line"><span class="keyword">typedef</span> <span class="class"><span class="keyword">struct</span> &#123;</span></span><br><span class="line">    <span class="type">char</span>** vocab;</span><br><span class="line">    <span class="type">float</span>*vocab_scores;</span><br><span class="line">    TokenIndex*sorted_vocab;</span><br><span class="line">    <span class="type">int</span> vocab_size;</span><br><span class="line">    <span class="type">unsigned</span> <span class="type">int</span> max_token_length;</span><br><span class="line">    <span class="type">unsigned</span> <span class="type">char</span> byte_pieces[<span class="number">512</span>]; <span class="comment">// stores all single-byte strings</span></span><br><span class="line marked">&#125; Tokenizer;</span><br></pre></td></tr></table></figure>

<figure class="highlight c"><figcaption><span>build_tokenizer</span><a target="_blank" rel="noopener" href="https://github.com/karpathy/llama2.c/blob/b3c4b6c3c4bbff42e5211293280307019368ccb5/run.c#L385">run.c</a></figcaption><table><tr><td class="gutter"><pre><span class="line">385</span><br><span class="line">386</span><br><span class="line">387</span><br><span class="line">388</span><br><span class="line">389</span><br><span class="line">390</span><br><span class="line">391</span><br><span class="line">392</span><br><span class="line">393</span><br><span class="line">394</span><br><span class="line">395</span><br><span class="line">396</span><br><span class="line">397</span><br><span class="line">398</span><br><span class="line">399</span><br><span class="line">400</span><br><span class="line">401</span><br><span class="line">402</span><br><span class="line">403</span><br><span class="line">404</span><br><span class="line">405</span><br><span class="line">406</span><br><span class="line">407</span><br><span class="line">408</span><br><span class="line">409</span><br></pre></td><td class="code"><pre><span class="line marked"><span class="type">void</span> <span class="title function_">build_tokenizer</span><span class="params">(Tokenizer*t, <span class="type">char</span>* tokenizer_path, <span class="type">int</span> vocab_size)</span> &#123;</span><br><span class="line">    <span class="comment">// i should have written the vocab_size into the tokenizer file... sigh</span></span><br><span class="line">    t-&gt;vocab_size = vocab_size;</span><br><span class="line">    <span class="comment">// malloc space to hold the scores and the strings</span></span><br><span class="line">    t-&gt;vocab = (<span class="type">char</span>**)<span class="built_in">malloc</span>(vocab_size *<span class="keyword">sizeof</span>(<span class="type">char</span>*));</span><br><span class="line">    t-&gt;vocab_scores = (<span class="type">float</span>*)<span class="built_in">malloc</span>(vocab_size* <span class="keyword">sizeof</span>(<span class="type">float</span>));</span><br><span class="line">    t-&gt;sorted_vocab = <span class="literal">NULL</span>; <span class="comment">// initialized lazily</span></span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>; i &lt; <span class="number">256</span>; i++) &#123;</span><br><span class="line">        t-&gt;byte_pieces[i * <span class="number">2</span>] = (<span class="type">unsigned</span> <span class="type">char</span>)i;</span><br><span class="line">        t-&gt;byte_pieces[i * <span class="number">2</span> + <span class="number">1</span>] = <span class="string">&#x27;\0&#x27;</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">// read in the file</span></span><br><span class="line">    FILE *file = fopen(tokenizer_path, <span class="string">&quot;rb&quot;</span>);</span><br><span class="line">    <span class="keyword">if</span> (!file) &#123; <span class="built_in">fprintf</span>(<span class="built_in">stderr</span>, <span class="string">&quot;couldn&#x27;t load %s\n&quot;</span>, tokenizer_path); <span class="built_in">exit</span>(EXIT_FAILURE); &#125;</span><br><span class="line">    <span class="keyword">if</span> (fread(&amp;t-&gt;max_token_length, <span class="keyword">sizeof</span>(<span class="type">int</span>), <span class="number">1</span>, file) != <span class="number">1</span>) &#123; <span class="built_in">fprintf</span>(<span class="built_in">stderr</span>, <span class="string">&quot;failed read\n&quot;</span>); <span class="built_in">exit</span>(EXIT_FAILURE); &#125;</span><br><span class="line">    <span class="type">int</span> len;</span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>; i &lt; vocab_size; i++) &#123;</span><br><span class="line">        <span class="keyword">if</span> (fread(t-&gt;vocab_scores + i, <span class="keyword">sizeof</span>(<span class="type">float</span>), <span class="number">1</span>, file) != <span class="number">1</span>) &#123; <span class="built_in">fprintf</span>(<span class="built_in">stderr</span>, <span class="string">&quot;failed read\n&quot;</span>); <span class="built_in">exit</span>(EXIT_FAILURE);&#125;</span><br><span class="line">        <span class="keyword">if</span> (fread(&amp;len, <span class="keyword">sizeof</span>(<span class="type">int</span>), <span class="number">1</span>, file) != <span class="number">1</span>) &#123; <span class="built_in">fprintf</span>(<span class="built_in">stderr</span>, <span class="string">&quot;failed read\n&quot;</span>); <span class="built_in">exit</span>(EXIT_FAILURE); &#125;</span><br><span class="line">        t-&gt;vocab[i] = (<span class="type">char</span>*)<span class="built_in">malloc</span>(len + <span class="number">1</span>);</span><br><span class="line">        <span class="keyword">if</span> (fread(t-&gt;vocab[i], len, <span class="number">1</span>, file) != <span class="number">1</span>) &#123; <span class="built_in">fprintf</span>(<span class="built_in">stderr</span>, <span class="string">&quot;failed read\n&quot;</span>); <span class="built_in">exit</span>(EXIT_FAILURE); &#125;</span><br><span class="line">        t-&gt;vocab[i][len] = <span class="string">&#x27;\0&#x27;</span>; <span class="comment">// add the string terminating token</span></span><br><span class="line">    &#125;</span><br><span class="line">    fclose(file);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h2 id="build-sampler"><a href="#build-sampler" class="headerlink" title="build_sampler"></a><code>build_sampler</code></h2><p>该版本将<code>logits</code>到文本字符串的采样过程重新做了封装，相关参数放到了<code>Sampler</code>结构体里。其次新增了<code>ProbIndex</code>以支持<strong>top-p</strong>采样方式，总共支持三种采样方式：</p>
<ol>
<li>贪心算法，argmax取最大值</li>
<li>随机采样</li>
<li>topp采样</li>
</ol>
<figure class="highlight c"><figcaption><span>Sampler</span><a target="_blank" rel="noopener" href="https://github.com/karpathy/llama2.c/blob/b3c4b6c3c4bbff42e5211293280307019368ccb5/run.c#L573">run.c</a></figcaption><table><tr><td class="gutter"><pre><span class="line">573</span><br><span class="line">574</span><br><span class="line">575</span><br><span class="line">576</span><br><span class="line">577</span><br><span class="line">578</span><br><span class="line">579</span><br><span class="line">580</span><br><span class="line">581</span><br><span class="line">582</span><br><span class="line">583</span><br><span class="line">584</span><br><span class="line">585</span><br><span class="line">586</span><br><span class="line">587</span><br><span class="line">588</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// ----------------------------------------------------------------------------</span></span><br><span class="line"><span class="comment">// The Sampler, which takes logits and returns a sampled token</span></span><br><span class="line"><span class="comment">// sampling can be done in a few ways: greedy argmax, sampling, top-p sampling</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">typedef</span> <span class="class"><span class="keyword">struct</span> &#123;</span></span><br><span class="line">    <span class="type">float</span> prob;</span><br><span class="line">    <span class="type">int</span> index;</span><br><span class="line marked">&#125; ProbIndex; <span class="comment">// struct used when sorting probabilities during top-p sampling</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">typedef</span> <span class="class"><span class="keyword">struct</span> &#123;</span></span><br><span class="line">    <span class="type">int</span> vocab_size;</span><br><span class="line">    ProbIndex* probindex; <span class="comment">// buffer used in top-p sampling</span></span><br><span class="line">    <span class="type">float</span> temperature;</span><br><span class="line">    <span class="type">float</span> topp;</span><br><span class="line">    <span class="type">unsigned</span> <span class="type">long</span> <span class="type">long</span> rng_state;</span><br><span class="line marked">&#125; Sampler;</span><br></pre></td></tr></table></figure>

<p>该函数初始化结构体及分配内存。<code>probindex</code>是维度为<code>(vocab_size,)</code>的<code>ProbIndex</code>结构体数组。</p>
<figure class="highlight c"><figcaption><span>build_sampler</span><a target="_blank" rel="noopener" href="https://github.com/karpathy/llama2.c/blob/b3c4b6c3c4bbff42e5211293280307019368ccb5/run.c#L667">run.c</a></figcaption><table><tr><td class="gutter"><pre><span class="line">667</span><br><span class="line">668</span><br><span class="line">669</span><br><span class="line">670</span><br><span class="line">671</span><br><span class="line">672</span><br><span class="line">673</span><br><span class="line">674</span><br></pre></td><td class="code"><pre><span class="line marked"><span class="type">void</span> <span class="title function_">build_sampler</span><span class="params">(Sampler*sampler, <span class="type">int</span> vocab_size, <span class="type">float</span> temperature, <span class="type">float</span> topp, <span class="type">unsigned</span> <span class="type">long</span> <span class="type">long</span> rng_seed)</span> &#123;</span><br><span class="line">    sampler-&gt;vocab_size = vocab_size;</span><br><span class="line">    sampler-&gt;temperature = temperature;</span><br><span class="line">    sampler-&gt;topp = topp;</span><br><span class="line">    sampler-&gt;rng_state = rng_seed;</span><br><span class="line">    <span class="comment">// buffer only used with nucleus sampling; may not need but it&#x27;s ~small</span></span><br><span class="line">    sampler-&gt;probindex = <span class="built_in">malloc</span>(sampler-&gt;vocab_size* <span class="keyword">sizeof</span>(ProbIndex));</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h2 id="encode"><a href="#encode" class="headerlink" title="encode"></a><code>encode</code></h2><p>Todo：</p>
<h2 id="decode"><a href="#decode" class="headerlink" title="decode"></a><code>decode</code></h2><p>解码过程就是<code>token:int -&gt; vocab:str</code>的转换，此处处理了空格及一些特殊符号。</p>
<figure class="highlight c"><figcaption><span>decode</span><a target="_blank" rel="noopener" href="https://github.com/karpathy/llama2.c/blob/b3c4b6c3c4bbff42e5211293280307019368ccb5/run.c#L418">run.c</a></figcaption><table><tr><td class="gutter"><pre><span class="line">418</span><br><span class="line">419</span><br><span class="line">420</span><br><span class="line">421</span><br><span class="line">422</span><br><span class="line">423</span><br><span class="line">424</span><br><span class="line">425</span><br><span class="line">426</span><br><span class="line">427</span><br><span class="line">428</span><br><span class="line">429</span><br></pre></td><td class="code"><pre><span class="line marked"><span class="type">char</span>* <span class="title function_">decode</span><span class="params">(Tokenizer* t, <span class="type">int</span> prev_token, <span class="type">int</span> token)</span> &#123;</span><br><span class="line">    <span class="type">char</span> *piece = t-&gt;vocab[token];</span><br><span class="line">    <span class="comment">// following BOS (1) token, sentencepiece decoder strips any leading whitespace (see PR #89)</span></span><br><span class="line">    <span class="keyword">if</span> (prev_token == <span class="number">1</span> &amp;&amp; piece[<span class="number">0</span>] == <span class="string">&#x27; &#x27;</span>) &#123; piece++; &#125;</span><br><span class="line">    <span class="comment">// careful, some tokens designate raw bytes, and look like e.g. &#x27;&lt;0x01&gt;&#x27;</span></span><br><span class="line">    <span class="comment">// parse this and convert and return the actual byte</span></span><br><span class="line">    <span class="type">unsigned</span> <span class="type">char</span> byte_val;</span><br><span class="line">    <span class="keyword">if</span> (<span class="built_in">sscanf</span>(piece, <span class="string">&quot;&lt;0x%02hhX&gt;&quot;</span>, &amp;byte_val) == <span class="number">1</span>) &#123;</span><br><span class="line">        piece = (<span class="type">char</span>*)t-&gt;byte_pieces + byte_val * <span class="number">2</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> piece;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h2 id="sample"><a href="#sample" class="headerlink" title="sample"></a><code>sample</code></h2><p>根据temperature、topp等超参数，将推理输出结果<code>logits</code>采样得到token id。根据超参数值的不同，支持三种采样方式：</p>
<ol>
<li><code>temperature == 0.0f</code>，则采用贪心算法，取argmax最大值，返回<code>sample_argmax</code>结果</li>
<li><code>topp&lt;=0 || topp&gt;=1</code>，随机采样，返回<code>sample_mult</code>结果</li>
<li>否则 top-p 采样，返回<code>sample_topp</code>结果</li>
</ol>
<figure class="highlight c"><figcaption><span>sample</span><a target="_blank" rel="noopener" href="https://github.com/karpathy/llama2.c/blob/b3c4b6c3c4bbff42e5211293280307019368ccb5/run.c#L691">run.c 691</a></figcaption><table><tr><td class="gutter"><pre><span class="line">691</span><br><span class="line">692</span><br><span class="line">693</span><br><span class="line">694</span><br><span class="line">695</span><br><span class="line">696</span><br><span class="line">697</span><br><span class="line">698</span><br><span class="line">699</span><br><span class="line">700</span><br><span class="line">701</span><br><span class="line">702</span><br><span class="line">703</span><br><span class="line">704</span><br><span class="line">705</span><br><span class="line">706</span><br><span class="line">707</span><br><span class="line">708</span><br><span class="line">709</span><br><span class="line">710</span><br><span class="line">711</span><br><span class="line">712</span><br><span class="line">713</span><br><span class="line">714</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">int</span> <span class="title function_">sample</span><span class="params">(Sampler*sampler, <span class="type">float</span>* logits)</span> &#123;</span><br><span class="line">    <span class="comment">// sample the token given the logits and some hyperparameters</span></span><br><span class="line">    <span class="type">int</span> next;</span><br><span class="line">    <span class="keyword">if</span> (sampler-&gt;temperature == <span class="number">0.0f</span>) &#123;</span><br><span class="line">        <span class="comment">// greedy argmax sampling: take the token with the highest probability</span></span><br><span class="line">        next = sample_argmax(logits, sampler-&gt;vocab_size);</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">        <span class="comment">// apply the temperature to the logits</span></span><br><span class="line">        <span class="keyword">for</span> (<span class="type">int</span> q=<span class="number">0</span>; q&lt;sampler-&gt;vocab_size; q++) &#123; logits[q] /= sampler-&gt;temperature; &#125;</span><br><span class="line">        <span class="comment">// apply softmax to the logits to get the probabilities for next token</span></span><br><span class="line">        softmax(logits, sampler-&gt;vocab_size);</span><br><span class="line">        <span class="comment">// flip a (float) coin (this is our source of entropy for sampling)</span></span><br><span class="line">        <span class="type">float</span> coin = random_f32(&amp;sampler-&gt;rng_state);</span><br><span class="line">        <span class="comment">// we sample from this distribution to get the next token</span></span><br><span class="line">        <span class="keyword">if</span> (sampler-&gt;topp &lt;= <span class="number">0</span> || sampler-&gt;topp &gt;= <span class="number">1</span>) &#123;</span><br><span class="line">            <span class="comment">// simply sample from the predicted probability distribution</span></span><br><span class="line">            next = sample_mult(logits, sampler-&gt;vocab_size, coin);</span><br><span class="line">        &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">            <span class="comment">// top-p (nucleus) sampling, clamping the least likely tokens to zero</span></span><br><span class="line">            next = sample_topp(logits, sampler-&gt;vocab_size, sampler-&gt;topp, sampler-&gt;probindex, coin);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> next;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h3 id="sample-argmax"><a href="#sample-argmax" class="headerlink" title="sample_argmax"></a><code>sample_argmax</code></h3><p>循环遍历所有元素，返回最大值的索引。</p>
<figure class="highlight c"><figcaption><span>sample_argmax</span><a target="_blank" rel="noopener" href="https://github.com/karpathy/llama2.c/blob/b3c4b6c3c4bbff42e5211293280307019368ccb5/run.c#L590">run.c 590</a></figcaption><table><tr><td class="gutter"><pre><span class="line">590</span><br><span class="line">591</span><br><span class="line">592</span><br><span class="line">593</span><br><span class="line">594</span><br><span class="line">595</span><br><span class="line">596</span><br><span class="line">597</span><br><span class="line">598</span><br><span class="line">599</span><br><span class="line">600</span><br><span class="line">601</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">int</span> <span class="title function_">sample_argmax</span><span class="params">(<span class="type">float</span>* probabilities, <span class="type">int</span> n)</span> &#123;</span><br><span class="line">    <span class="comment">// return the index that has the highest probability</span></span><br><span class="line">    <span class="type">int</span> max_i = <span class="number">0</span>;</span><br><span class="line">    <span class="type">float</span> max_p = probabilities[<span class="number">0</span>];</span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">1</span>; i &lt; n; i++) &#123;</span><br><span class="line">        <span class="keyword">if</span> (probabilities[i] &gt; max_p) &#123;</span><br><span class="line">            max_i = i;</span><br><span class="line">            max_p = probabilities[i];</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> max_i;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h3 id="sample-mult"><a href="#sample-mult" class="headerlink" title="sample_mult"></a><code>sample_mult</code></h3><p>累加结果超过给定随机值，就返回当前的索引。</p>
<figure class="highlight c"><figcaption><span>sample_mult</span><a target="_blank" rel="noopener" href="https://github.com/karpathy/llama2.c/blob/b3c4b6c3c4bbff42e5211293280307019368ccb5/run.c#L603">run.c 603</a></figcaption><table><tr><td class="gutter"><pre><span class="line">603</span><br><span class="line">604</span><br><span class="line">605</span><br><span class="line">606</span><br><span class="line">607</span><br><span class="line">608</span><br><span class="line">609</span><br><span class="line">610</span><br><span class="line">611</span><br><span class="line">612</span><br><span class="line">613</span><br><span class="line">614</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">int</span> <span class="title function_">sample_mult</span><span class="params">(<span class="type">float</span>* probabilities, <span class="type">int</span> n, <span class="type">float</span> coin)</span> &#123;</span><br><span class="line">    <span class="comment">// sample index from probabilities (they must sum to 1!)</span></span><br><span class="line">    <span class="comment">// coin is a random number in [0, 1), usually from random_f32()</span></span><br><span class="line">    <span class="type">float</span> cdf = <span class="number">0.0f</span>;</span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>; i &lt; n; i++) &#123;</span><br><span class="line">        cdf += probabilities[i];</span><br><span class="line">        <span class="keyword">if</span> (coin &lt; cdf) &#123;</span><br><span class="line">            <span class="keyword">return</span> i;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> n - <span class="number">1</span>; <span class="comment">// in case of rounding errors</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h3 id="sample-topp"><a href="#sample-topp" class="headerlink" title="sample_topp"></a><code>sample_topp</code></h3>
$$
topp(p) = \max{k : \sum_{i=1}^{k} P(w_i) \leq p}
$$


<p>计算过程：</p>
<ol>
<li>首先设定阈值为 <code>float cutoff = (1.0f - topp) / (n-1)</code></li>
<li>初筛，挑选出概率大于阈值的所有token，其它的舍弃，结果存入<code>probindex</code>数组中。</li>
<li>再按照概率值降序排列</li>
<li>细筛，从前往后累加，得到<strong>不大于</strong><code>topp</code>的和<code>cumulative_prob</code>及对应的所以<code>last_idx</code>，然后截取<code>last_idx</code>之前的部分作为候选列表。</li>
<li>采样，设定阈值<code>float r = coin * cumulative_prob</code>，从候选列表里找累加概率值不大于阈值<code>r</code>的token返回</li>
</ol>
<h1 id="关键数据结构"><a href="#关键数据结构" class="headerlink" title="关键数据结构"></a>关键数据结构</h1><h2 id="kv-cache内存结构"><a href="#kv-cache内存结构" class="headerlink" title="kv_cache内存结构"></a><code>kv_cache</code>内存结构</h2><p>首先kv cache每个head的维度必须要与query保持一致才能做点积所以都是：<code>head_dim=dim/n_heads</code>，其次是kv各自有<code>n_kv_heads</code>个head，所以很容易得出kv cache的维度为<code>(n_layers, seq_len, n_kv_heads, head_dim)</code>。那么从layer层面看（尚未区分多头）的话，每个token对应kv的维度就是<code>kv_dim = n_kv_heads * head_dim</code>，其内存结构示意图如下。这跟query（<code>dim</code>）是不一样的，切记切记。相应的，$W_k$, $W_v$权重矩阵维度也变成了<code>(layer, dim, kv_dim)</code>跟Query不一样。切记切记。所以我们可以看到，MQA&#x2F;GQA减少了kv head，实际上是缩小了两个权重矩阵的大小，同时有另外一层隐藏的含义，就是每个token对应的kv dim维度也变小了，也就是信息容量更少了，性能肯定受影响。</p>
<blockquote>
<p>注：也许好奇，GQA为何kv总的维度不合q保持一致都是<code>dim</code>，因为<code>head_dim</code>和<code>dim</code>只能有一个相同，否则head数量就得一致，也就是MHA了。</p>
</blockquote>
<p>接下来，因为<code>n_kv_heads&lt;=n_heads</code>所以必然有<code>kv_dim</code>是要被多个相邻的query head共享的，举个例子，假如<code>n_heads=6,n_kv_heads=3</code>,则<code>kv_dim = 3 * head_dim</code>, 那么每个kv head会被2个query head共享。所以6个query head对应的kv head索引分别是：(0, 0, 1, 1, 2, 2)，每个长度是<code>head_dim</code>所以加起来正好是<code>kv_dim</code>，相邻两个query对应的kv head是同一个。</p>
<table>
<thead>
<tr>
<th>layer-0</th>
<th></th>
<th></th>
<th></th>
<th>layer-1</th>
<th></th>
<th></th>
<th></th>
<th>…</th>
<th>layer-n</th>
<th></th>
<th></th>
<th></th>
</tr>
</thead>
<tbody><tr>
<td>token-0</td>
<td>token-1</td>
<td>…</td>
<td>token-k</td>
<td>token-0</td>
<td>token-1</td>
<td>…</td>
<td>token-k</td>
<td>…</td>
<td>token-0</td>
<td>token-1</td>
<td>…</td>
<td>token-k</td>
</tr>
<tr>
<td><code>(kv_dim,)</code></td>
<td><code>(kv_dim,)</code></td>
<td>…</td>
<td><code>(kv_dim,)</code></td>
<td><code>(kv_dim,)</code></td>
<td><code>(kv_dim,)</code></td>
<td>…</td>
<td><code>(kv_dim,)</code></td>
<td>…</td>
<td><code>(kv_dim,)</code></td>
<td><code>(kv_dim,)</code></td>
<td>…</td>
<td><code>(kv_dim,)</code></td>
</tr>
</tbody></table>
<h2 id="tokenizer文件"><a href="#tokenizer文件" class="headerlink" title="tokenizer文件"></a><code>tokenizer</code>文件</h2><p>该文件结构与之前版本有差异，文件开头增加了一个4字节的<code>int</code>值<code>max_token_length</code>，统计了所有token最大的长度。其次是遍历每个token的内容，包括新增的一个<code>float</code>的<code>vocab_scores</code>，然后是4个字节<code>int</code>型的token长度，紧接着是token内容（不包括结束符的字符串），所有token一个挨一个存储。内存结构示意图如下。</p>
<blockquote>
<p><code>vocab_size</code> 存储在 <code>checkpoint</code> 文件里。</p>
</blockquote>
<table>
<thead>
<tr>
<th>文件头</th>
<th>token-0</th>
<th></th>
<th></th>
<th>token-1</th>
<th></th>
<th></th>
<th>…</th>
<th>token-n</th>
<th></th>
<th></th>
</tr>
</thead>
<tbody><tr>
<td>max_token_length</td>
<td>vocab_scores</td>
<td>token len</td>
<td>token content</td>
<td>vocab_scores</td>
<td>token len</td>
<td>token content</td>
<td>…</td>
<td>vocab_scores</td>
<td>token len</td>
<td>token content</td>
</tr>
<tr>
<td>27</td>
<td>0.0</td>
<td>1</td>
<td>l</td>
<td>0.0</td>
<td>4</td>
<td>like</td>
<td>…</td>
<td>0.0</td>
<td>11</td>
<td>suggestions</td>
</tr>
</tbody></table>
<h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h1><p><code>MQA</code>、<code>GQA</code>是最近出现的技术，原本的注意力机制是<code>Multi-Head Attention(MHA)</code>，每个query head对应一个key、value的head，最简化的情况是<code>MQA</code>，所有query head共享一组key、value head，$W_k$和$W_v$矩阵维度缩小为原来的<code>1/n_heads</code>，内存占用和推理速度提升非常明显，但相应的性能下降了（每个token对应的维度也变为原来的<code>1/n_heads</code>，信息被严重压缩）。折中的方案就是<code>GQA</code>，处于两者中间，且可以配置<code>n_kv_heads</code>大小，自由权衡取舍。</p>
<blockquote><p>GQA: Training Generalized Multi-Query Transformer Models from Multi-Head Checkpoints</p>
<footer><strong>Joshua Ainslie</strong><cite><a target="_blank" rel="noopener" href="https://arxiv.org/abs/2305.13245v1">GQA</a></cite></footer></blockquote>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">文章作者: </span><span class="post-copyright-info"><a href="https://waterdropw.github.io">xbw</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">文章链接: </span><span class="post-copyright-info"><a href="https://waterdropw.github.io/2024/04/17/Dive-into-Transformer-simpliest-implementation-llama2-c-b3c4b6c/">https://waterdropw.github.io/2024/04/17/Dive-into-Transformer-simpliest-implementation-llama2-c-b3c4b6c/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="https://waterdropw.github.io" target="_blank">xbw's blog</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/AI/">AI</a><a class="post-meta__tags" href="/tags/transformer/">transformer</a><a class="post-meta__tags" href="/tags/self-attention/">self-attention</a></div><div class="post_share"><div class="social-share" data-image="/images/waterdrop.jpeg" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/2024/04/17/Dive-into-Transformer-simpliest-implementation-llama2-c-4e23ad8/" title="极简`Transformer`实现(基于`llama2.c`commit-4e23ad 23'7/27)"><div class="cover" style="background: var(--default-bg-color)"></div><div class="pagination-info"><div class="label">上一篇</div><div class="prev_info">极简`Transformer`实现(基于`llama2.c`commit-4e23ad 23'7/27)</div></div></a></div><div class="next-post pull-right"><a href="/2023/09/28/NCCL-Debug/" title="多机多卡训练：NCCL Debug"><div class="cover" style="background: var(--default-bg-color)"></div><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">多机多卡训练：NCCL Debug</div></div></a></div></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span>相关推荐</span></div><div class="relatedPosts-list"><div><a href="/2024/04/17/Dive-into-Transformer-simpliest-implementation-llama2-c-4e23ad8/" title="极简&#96;Transformer&#96;实现(基于&#96;llama2.c&#96;commit-4e23ad 23&#39;7&#x2F;27)"><div class="cover" style="background: var(--default-bg-color)"></div><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-04-17</div><div class="title">极简&#96;Transformer&#96;实现(基于&#96;llama2.c&#96;commit-4e23ad 23&#39;7&#x2F;27)</div></div></a></div><div><a href="/2023/09/28/LLaMA/" title="论文摘要-LLaMA"><div class="cover" style="background: var(--default-bg-color)"></div><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2023-09-28</div><div class="title">论文摘要-LLaMA</div></div></a></div><div><a href="/2023/09/28/NCCL-Debug/" title="多机多卡训练：NCCL Debug"><div class="cover" style="background: var(--default-bg-color)"></div><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2023-09-28</div><div class="title">多机多卡训练：NCCL Debug</div></div></a></div></div></div><hr class="custom-hr"/><div id="post-comment"><div class="comment-head"><div class="comment-headline"><i class="fas fa-comments fa-fw"></i><span> 评论</span></div></div><div class="comment-wrap"><div><div id="gitalk-container"></div></div></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="is-center"><div class="avatar-img"><img src="/images/waterdrop.jpeg" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info__name">xbw</div><div class="author-info__description">技术博客，杂记，随笔</div></div><div class="card-info-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">12</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">15</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">15</div></a></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/waterdropw"><i class="fab fa-github"></i><span>Follow Me</span></a><div class="card-info-social-icons is-center"><a class="social-icon" href="https://github.com/waterdropw" target="_blank" title="Github"><i class="fab fa-github" style="color: #24292e;"></i></a><a class="social-icon" href="mailto:xiaobin.wee@gmail.com" target="_blank" title="Email"><i class="fas fa-envelope" style="color: #4a7dbe;"></i></a></div></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>公告</span></div><div class="announcement_content">欢迎来到三体世界，我是水滴💧</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#%E6%89%A7%E8%A1%8C%E6%B5%81%E7%A8%8B"><span class="toc-number">1.</span> <span class="toc-text">执行流程</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#main-%E6%89%A7%E8%A1%8C%E6%B5%81%E7%A8%8B"><span class="toc-number">1.1.</span> <span class="toc-text">main 执行流程</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#generate"><span class="toc-number">1.2.</span> <span class="toc-text">generate</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#chat"><span class="toc-number">1.3.</span> <span class="toc-text">chat</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#forward"><span class="toc-number">1.4.</span> <span class="toc-text">forward</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E5%85%B6%E5%AE%83%E5%87%BD%E6%95%B0"><span class="toc-number">2.</span> <span class="toc-text">其它函数</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#build-transformer"><span class="toc-number">2.1.</span> <span class="toc-text">build_transformer</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#build-tokenizer"><span class="toc-number">2.2.</span> <span class="toc-text">build_tokenizer</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#build-sampler"><span class="toc-number">2.3.</span> <span class="toc-text">build_sampler</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#encode"><span class="toc-number">2.4.</span> <span class="toc-text">encode</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#decode"><span class="toc-number">2.5.</span> <span class="toc-text">decode</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#sample"><span class="toc-number">2.6.</span> <span class="toc-text">sample</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#sample-argmax"><span class="toc-number">2.6.1.</span> <span class="toc-text">sample_argmax</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#sample-mult"><span class="toc-number">2.6.2.</span> <span class="toc-text">sample_mult</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#sample-topp"><span class="toc-number">2.6.3.</span> <span class="toc-text">sample_topp</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E5%85%B3%E9%94%AE%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84"><span class="toc-number">3.</span> <span class="toc-text">关键数据结构</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#kv-cache%E5%86%85%E5%AD%98%E7%BB%93%E6%9E%84"><span class="toc-number">3.1.</span> <span class="toc-text">kv_cache内存结构</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#tokenizer%E6%96%87%E4%BB%B6"><span class="toc-number">3.2.</span> <span class="toc-text">tokenizer文件</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E6%80%BB%E7%BB%93"><span class="toc-number">4.</span> <span class="toc-text">总结</span></a></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2025/11/21/Ant-Robot-Research/" title="Ant Robot Research">Ant Robot Research</a><time datetime="2025-11-21T02:43:15.000Z" title="发表于 2025-11-21 10:43:15">2025-11-21</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2024/04/17/Dive-into-Transformer-simpliest-implementation-llama2-c-4e23ad8/" title="极简`Transformer`实现(基于`llama2.c`commit-4e23ad 23'7/27)">极简`Transformer`实现(基于`llama2.c`commit-4e23ad 23'7/27)</a><time datetime="2024-04-17T02:34:26.000Z" title="发表于 2024-04-17 10:34:26">2024-04-17</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2024/04/17/Dive-into-Transformer-simpliest-implementation-llama2-c-b3c4b6c/" title="极简`Transformer`实现(基于`llama2.c`commit-b3c4b6c 24'2/13)">极简`Transformer`实现(基于`llama2.c`commit-b3c4b6c 24'2/13)</a><time datetime="2024-04-17T02:34:26.000Z" title="发表于 2024-04-17 10:34:26">2024-04-17</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2023/09/28/NCCL-Debug/" title="多机多卡训练：NCCL Debug">多机多卡训练：NCCL Debug</a><time datetime="2023-09-28T08:05:10.000Z" title="发表于 2023-09-28 16:05:10">2023-09-28</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2023/09/28/LLaMA/" title="论文摘要-LLaMA">论文摘要-LLaMA</a><time datetime="2023-09-28T06:48:11.000Z" title="发表于 2023-09-28 14:48:11">2023-09-28</time></div></div></div></div></div></div></main><footer id="footer"><div id="footer-wrap"><div class="copyright">&copy;2020 - 2025 By xbw</div><div class="framework-info"><span>框架 </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>主题 </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside_config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><button id="chat-btn" type="button" title="聊天"><i class="fas fa-sms"></i></button><a id="to_comment" href="#post-comment" title="直达评论"><i class="fas fa-comments"></i></a><button id="go-up" type="button" title="回到顶部"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox/fancybox.umd.min.js"></script><script>function panguFn () {
  if (typeof pangu === 'object') pangu.autoSpacingPage()
  else {
    getScript('https://cdn.jsdelivr.net/npm/pangu/dist/browser/pangu.min.js')
      .then(() => {
        pangu.autoSpacingPage()
      })
  }
}

function panguInit () {
  if (false){
    GLOBAL_CONFIG_SITE.isPost && panguFn()
  } else {
    panguFn()
  }
}

document.addEventListener('DOMContentLoaded', panguInit)</script><div class="js-pjax"><script>if (!window.MathJax) {
  window.MathJax = {
    tex: {
      inlineMath: [['$', '$'], ['\\(', '\\)']],
      tags: 'ams'
    },
    chtml: {
      scale: 1.1
    },
    options: {
      renderActions: {
        findScript: [10, doc => {
          for (const node of document.querySelectorAll('script[type^="math/tex"]')) {
            const display = !!node.type.match(/; *mode=display/)
            const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display)
            const text = document.createTextNode('')
            node.parentNode.replaceChild(text, node)
            math.start = {node: text, delim: '', n: 0}
            math.end = {node: text, delim: '', n: 0}
            doc.math.push(math)
          }
        }, '']
      }
    }
  }
  
  const script = document.createElement('script')
  script.src = 'https://cdn.jsdelivr.net/npm/mathjax/es5/tex-mml-chtml.min.js'
  script.id = 'MathJax-script'
  script.async = true
  document.head.appendChild(script)
} else {
  MathJax.startup.document.state(0)
  MathJax.texReset()
  MathJax.typesetPromise()
}</script><script>function loadGitalk () {
  function initGitalk () {
    var gitalk = new Gitalk(Object.assign({
      clientID: '877679214f203d8523ee',
      clientSecret: '45f0f9eb775009a45487e3330795c0ac1e3f08b3',
      repo: 'waterdropw.github.io',
      owner: 'waterdropw',
      admin: ['waterdropw'],
      id: '0e4d37c1e38ece0252d0d60dc7831926',
      updateCountCallback: commentCount
    },null))

    gitalk.render('gitalk-container')
  }

  if (typeof Gitalk === 'function') initGitalk()
  else {
    getCSS('https://cdn.jsdelivr.net/npm/gitalk/dist/gitalk.min.css')
    getScript('https://cdn.jsdelivr.net/npm/gitalk/dist/gitalk.min.js').then(initGitalk)
  }
}

function commentCount(n){
  let isCommentCount = document.querySelector('#post-meta .gitalk-comment-count')
  if (isCommentCount) {
    isCommentCount.textContent= n
  }
}

if ('Gitalk' === 'Gitalk' || !true) {
  if (true) btf.loadComment(document.getElementById('gitalk-container'), loadGitalk)
  else loadGitalk()
} else {
  function loadOtherComment () {
    loadGitalk()
  }
}</script></div><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/dist/activate-power-mode.min.js"></script><script>POWERMODE.colorful = true;
POWERMODE.shake = true;
POWERMODE.mobile = false;
document.body.addEventListener('input', POWERMODE);
</script><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div></body></html>