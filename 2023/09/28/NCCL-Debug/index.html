<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0,viewport-fit=cover"><title>多机多卡训练：NCCL Debug | waterdropw 的博客</title><meta name="author" content="waterdropw"><meta name="copyright" content="waterdropw"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="大模型因为参数量巨大，即使是Finetune也只能在多卡GPU的机器上训练（全精度），如果是A100 8卡40GB机器，用上DeepSpeed的各种优化之后勉强能训3B模型，7B模型训不了，必须要多机多卡才行。这里记录一下早期探索，使用裸机环境配置多机多卡来跑大模型训练遇到的一些问题。 多机多卡训练需要一个高效的通信框架来协调多个设备之间的数据传输和计算任务。常见的通信框架包括MPI、NCCL等。">
<meta property="og:type" content="article">
<meta property="og:title" content="多机多卡训练：NCCL Debug">
<meta property="og:url" content="https://three-body.com.cn/2023/09/28/NCCL-Debug/index.html">
<meta property="og:site_name" content="waterdropw 的博客">
<meta property="og:description" content="大模型因为参数量巨大，即使是Finetune也只能在多卡GPU的机器上训练（全精度），如果是A100 8卡40GB机器，用上DeepSpeed的各种优化之后勉强能训3B模型，7B模型训不了，必须要多机多卡才行。这里记录一下早期探索，使用裸机环境配置多机多卡来跑大模型训练遇到的一些问题。 多机多卡训练需要一个高效的通信框架来协调多个设备之间的数据传输和计算任务。常见的通信框架包括MPI、NCCL等。">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://three-body.com.cn/images/waterdrop.jpeg">
<meta property="article:published_time" content="2023-09-28T08:05:10.000Z">
<meta property="article:modified_time" content="2024-04-16T09:05:55.699Z">
<meta property="article:author" content="waterdropw">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://three-body.com.cn/images/waterdrop.jpeg"><link rel="shortcut icon" href="/img/favicon.png"><link rel="canonical" href="https://three-body.com.cn/2023/09/28/NCCL-Debug/index.html"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox/fancybox.min.css" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = {
  root: '/',
  algolia: undefined,
  localSearch: undefined,
  translate: undefined,
  noticeOutdate: undefined,
  highlight: {"plugin":"highlighjs","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '',
  dateSuffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'fancybox',
  Snackbar: undefined,
  source: {
    justifiedGallery: {
      js: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery/dist/fjGallery.min.js',
      css: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery/dist/fjGallery.min.css'
    }
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isAnchor: false,
  percent: {
    toc: true,
    rightside: false,
  },
  autoDarkmode: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: '多机多卡训练：NCCL Debug',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2024-04-16 17:05:55'
}</script><noscript><style type="text/css">
  #nav {
    opacity: 1
  }
  .justified-gallery img {
    opacity: 1
  }

  #recent-posts time,
  #post-meta time {
    display: inline !important
  }
</style></noscript><script>(win=>{
    win.saveToLocal = {
      set: function setWithExpiry(key, value, ttl) {
        if (ttl === 0) return
        const now = new Date()
        const expiryDay = ttl * 86400000
        const item = {
          value: value,
          expiry: now.getTime() + expiryDay,
        }
        localStorage.setItem(key, JSON.stringify(item))
      },

      get: function getWithExpiry(key) {
        const itemStr = localStorage.getItem(key)

        if (!itemStr) {
          return undefined
        }
        const item = JSON.parse(itemStr)
        const now = new Date()

        if (now.getTime() > item.expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return item.value
      }
    }
  
    win.getScript = url => new Promise((resolve, reject) => {
      const script = document.createElement('script')
      script.src = url
      script.async = true
      script.onerror = reject
      script.onload = script.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        script.onload = script.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(script)
    })
  
    win.getCSS = (url,id = false) => new Promise((resolve, reject) => {
      const link = document.createElement('link')
      link.rel = 'stylesheet'
      link.href = url
      if (id) link.id = id
      link.onerror = reject
      link.onload = link.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        link.onload = link.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(link)
    })
  
      win.activateDarkMode = function () {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = function () {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
          if (t === 'dark') activateDarkMode()
          else if (t === 'light') activateLightMode()
        
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
    const detectApple = () => {
      if(/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
    })(window)</script><!-- hexo injector head_end start -->
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css">

<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/hexo-math@4.0.0/dist/style.css">
<!-- hexo injector head_end end --><meta name="generator" content="Hexo 6.3.0"><link rel="alternate" href="/atom.xml" title="waterdropw 的博客" type="application/atom+xml">
</head><body><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src="/images/waterdrop.jpeg" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="sidebar-site-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">9</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">0</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">0</div></a></div><hr class="custom-hr"/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 归档</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 目录</span></a></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fas fa-list"></i><span> 娱乐</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/music/"><i class="fa-fw fas fa-music"></i><span> 音乐</span></a></li><li><a class="site-page child" href="/movies/"><i class="fa-fw fas fa-video"></i><span> 电影</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> 链接</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url('https://i.loli.net/2020/05/01/gkihqEjXxJ5UZ1C.jpg')"><nav id="nav"><span id="blog-info"><a href="/" title="waterdropw 的博客"><span class="site-name">waterdropw 的博客</span></a></span><div id="menus"><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 归档</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 目录</span></a></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fas fa-list"></i><span> 娱乐</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/music/"><i class="fa-fw fas fa-music"></i><span> 音乐</span></a></li><li><a class="site-page child" href="/movies/"><i class="fa-fw fas fa-video"></i><span> 电影</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> 链接</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于</span></a></div></div><div id="toggle-menu"><a class="site-page" href="javascript:void(0);"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">多机多卡训练：NCCL Debug</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2023-09-28T08:05:10.000Z" title="发表于 2023-09-28 16:05:10">2023-09-28</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2024-04-16T09:05:55.699Z" title="更新于 2024-04-16 17:05:55">2024-04-16</time></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title="多机多卡训练：NCCL Debug"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">阅读量:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><p>大模型因为参数量巨大，即使是Finetune也只能在多卡GPU的机器上训练（全精度），如果是A100 8卡40GB机器，用上DeepSpeed的各种优化之后勉强能训3B模型，7B模型训不了，必须要多机多卡才行。这里记录一下早期探索，使用裸机环境配置多机多卡来跑大模型训练遇到的一些问题。</p>
<p>多机多卡训练需要一个高效的通信框架来协调多个设备之间的数据传输和计算任务。常见的通信框架包括MPI、NCCL等。同时，多机多卡训练还需要一些额外的技术支持，如数据并行化、模型并行化等，以便将计算和存储任务分配到不同的设备上。</p>
<p>虽然多机多卡训练可以大大加速深度学习模型的训练速度，但也面临一些挑战，如设备故障、通信延迟等。因此，在应用多机多卡训练时需要谨慎选择合适的硬件设备和软件工具，并进行充分测试和优化。</p>
<span id="more"></span>

<h2 id="测试一"><a href="#测试一" class="headerlink" title="测试一"></a>测试一</h2><h3 id="环境配置"><a href="#环境配置" class="headerlink" title="环境配置"></a>环境配置</h3><p>两台主机加入 swarm worker，docker 指定overlay network</p>
<p>容器启动之后，需要手动启动ssh</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">/etc/init.d/ssh start</span><br></pre></td></tr></table></figure>

<p>运行 DeepSpeed-chat 多机训练</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">NCCL_DEBUG_SUBSYS=ALL NCCL_IB_DISABLE=1 NCCL_DEBUG=INFO python train.py --actor-model facebook/opt-1.3b --reward-model facebook/opt-350m --deployment-type multi_node --actor-zero-stage 3 --output-dir /data/opt-1.3b-multi-node/ --hostfile hostfile</span><br></pre></td></tr></table></figure>

<h3 id="log-分析"><a href="#log-分析" class="headerlink" title="log 分析"></a>log 分析</h3><p>错误如下：socket 连接错误</p>
<img src="/images/nccl/fig1.png" class="" width="1400" height="428" title="socket error">

<p>GPU 不支持 NCCL IB，因此设置<code>NCCL_IB_DISABLE=1</code> 禁用掉，多机间通信将透过socket 建立连接，并且使用的eth0，eth1 是容器的网络配置</p>
<img src="/images/nccl/fig2.png" class="" width="865" height="188" title="NCCL IB">

<p>容器内 ifconfig 结果：</p>
<img src="/images/nccl/fig3.png" class="" width="574" height="402" title="container ifconfig">

<p>主机的ifconfig：</p>
<img src="/images/nccl/fig4.png" class="" width="593" height="600" title="host ifconfig">

<p>如下提示，说明两个网络上 RDMA 也不能用</p>
<img src="/images/nccl/fig5.png" class="" width="813" height="34" title="RDMA">
<img src="/images/nccl/fig6.png" class="" width="797" height="234" title="NCCL INFO">
<img src="/images/nccl/fig7.png" class="" width="770" height="236" title="NCCL INFO">

<h2 id="测试二"><a href="#测试二" class="headerlink" title="测试二"></a>测试二</h2><h3 id="环境配置-1"><a href="#环境配置-1" class="headerlink" title="环境配置"></a>环境配置</h3><p>两台主机未加入swarm（docker swarm leave），docker容器指定为host 网络，docker启动后ssh port 6000 修改为22</p>
<p>运行 DeepSpeed-chat 多机训练</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">NCCL_DEBUG_SUBSYS=ALL NCCL_IB_DISABLE=1 NCCL_DEBUG=INFO NCCL_SOCKET_IFNAME=eth0 python train.py --actor-model facebook/opt-1.3b --reward-model facebook/opt-350m --deployment-type multi_node --actor-zero-stage 3 --output-dir /data/opt-1.3b-multi-node/ --hostfile hostfile</span><br></pre></td></tr></table></figure>

<h3 id="log-分析-1"><a href="#log-分析-1" class="headerlink" title="log 分析"></a>log 分析</h3><img src="/images/nccl/fig8.png" class="" width="1358" height="449" title="socket error">
<img src="/images/nccl/fig8.png" class="" width="1247" height="26" title="socket error">
<img src="/images/nccl/fig8.png" class="" width="1360" height="71" title="socket error">
<img src="/images/nccl/fig8.png" class="" width="1121" height="63" title="socket error">

<h2 id="测试三"><a href="#测试三" class="headerlink" title="测试三"></a>测试三</h2><h3 id="环境配置-2"><a href="#环境配置-2" class="headerlink" title="环境配置"></a>环境配置</h3><p>两台主机未加入swarm（docker swarm leave），docker容器指定为host 网络，docker启动后ssh port 6000 修改为22</p>
<p>torchrun 运行 BELLE&#x2F;train 多机训练，排除DeepSpeed的影响</p>
<p>node1 （182）上执行：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">OMP_NUM_THREADS=8 torchrun --node_rank=0 --master_addr=192.168.0.8 --master_port=29500 --nnodes=2 --nproc_per_node=8 finetune.py --model_config_file run_config/Bloom_config.json --lora_hyperparams_file run_config/lora_hyperparams_bloom.json --use_lora</span><br></pre></td></tr></table></figure>

<p>node2 （188）上执行：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">OMP_NUM_THREADS=8 torchrun --node_rank=1 --master_addr=192.168.0.16 --master_port=29500 --nnodes=2 --nproc_per_node=8 finetune.py --model_config_file run_config/Bloom_config.json --lora_hyperparams_file run_config/lora_hyperparams_bloom.json --use_lora</span><br></pre></td></tr></table></figure>

<h2 id="测试四"><a href="#测试四" class="headerlink" title="测试四"></a>测试四</h2><h3 id="环境配置-3"><a href="#环境配置-3" class="headerlink" title="环境配置"></a>环境配置</h3><p>两台主机未加入swarm（docker swarm leave），docker容器指定为host 网络，docker启动后ssh port 6000 修改为22</p>
<p>运行 nccl test 命令</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">NCCL_DEBUG_SUBSYS=ALL NCCL_DEBUG=INFO NCCL_IB_DISABLE=1 NCCL_SOCKET_IFNAME=eth0 mpirun --allow-run-as-root  -np 16 -H 192.168.0.8:54321,192.168.0.16:54321 ./build/all_gather_perf  -b 8 -e  128M -f 2 -g 8 2&gt;&amp;1 |<span class="built_in">tee</span> ib.log</span><br></pre></td></tr></table></figure>

<h2 id="最终解决方案"><a href="#最终解决方案" class="headerlink" title="最终解决方案"></a>最终解决方案</h2><p>分析 nccl_test log发现，多机之间的连接还是因为防火墙和iptables 的影响，索性关闭防火墙</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#!/usr/bin/env bash</span></span><br><span class="line"></span><br><span class="line">iptables -t nat -P PREROUTING ACCEPT</span><br><span class="line">iptables -t nat -P POSTROUTING ACCEPT</span><br><span class="line">iptables -t nat -P OUTPUT ACCEPT</span><br><span class="line">iptables -t nat -F</span><br><span class="line">iptables -t nat -X</span><br><span class="line">iptables -t mangle -P PREROUTING ACCEPT</span><br><span class="line">iptables -t mangle -P INPUT ACCEPT</span><br><span class="line">iptables -t mangle -P FORWARD ACCEPT</span><br><span class="line">iptables -t mangle -P OUTPUT ACCEPT</span><br><span class="line">iptables -t mangle -P POSTROUTING ACCEPT</span><br><span class="line">iptables -t mangle -F</span><br><span class="line">iptables -t mangle -X</span><br><span class="line">iptables -t filter -P INPUT ACCEPT</span><br><span class="line">iptables -t filter -P FORWARD ACCEPT</span><br><span class="line">iptables -t filter -P OUTPUT ACCEPT</span><br><span class="line">iptables -t filter -F</span><br><span class="line">iptables -t filter -X</span><br><span class="line"></span><br><span class="line">ufw <span class="built_in">disable</span></span><br></pre></td></tr></table></figure>

<p>双机训练跑通，中途worker的ssh 连接中断了，稳定性有待验证</p>
<details>
  <summary>点击展开</summary>
  <pre><code>
[2023-04-17 17:13:37,910] [INFO] [runner.py:446:main] Using IP address of 192.168.0.8 for node a182
[2023-04-17 17:13:37,911] [INFO] [multinode_runner.py:70:get_cmd] Running on the following workers: a182,a188
[2023-04-17 17:13:37,911] [INFO] [runner.py:540:main] cmd = pdsh -S -f 1024 -w a182,a188 export NCCL_VERSION=2.16.5; export NCCL_SOCKET_IFNAME=eth0; export PYTHONIOENCODING=utf-8; export NCCL_IB_DISABLE=1; export PYTHONPATH=/data/repos/DeepSpeedExamples/applications/DeepSpeed-Chat/training/step1_supervised_finetuning;  cd /data/repos/DeepSpeedExamples/applications/DeepSpeed-Chat/training/step1_supervised_finetuning; /usr/bin/python -u -m deepspeed.launcher.launch --world_info=eyJhMTgyIjogWzAsIDEsIDIsIDMsIDQsIDUsIDYsIDddLCAiYTE4OCI6IFswLCAxLCAyLCAzLCA0LCA1LCA2LCA3XX0= --node_rank=%n --master_addr=192.168.0.8 --master_port=29500 main.py --data_path 'Dahoas/rm-static' 'Dahoas/full-hh-rlhf' 'Dahoas/synthetic-instruct-gptj-pairwise' 'yitingxie/rlhf-reward-datasets' 'openai/webgpt_comparisons' 'stanfordnlp/SHP' --data_split '2,4,4' --model_name_or_path 'facebook/opt-1.3b' --per_device_train_batch_size '4' --per_device_eval_batch_size '4' --max_seq_len '512' --learning_rate '1e-4' --weight_decay '0.1' --num_train_epochs '2' --gradient_accumulation_steps '1' --lr_scheduler_type 'cosine' --num_warmup_steps '0' --seed '1234' --gradient_checkpointing --zero_stage '3' --lora_dim '128' --lora_module_name 'decoder.layers.' --deepspeed --output_dir '/data/opt-1.3b-multi-node/actor-models/1.3b'
a182: [2023-04-17 17:13:43,896] [INFO] [launch.py:222:main] 0 NCCL_VERSION=2.16.5
a182: [2023-04-17 17:13:43,897] [INFO] [launch.py:222:main] 0 NCCL_SOCKET_IFNAME=eth0
a182: [2023-04-17 17:13:43,897] [INFO] [launch.py:222:main] 0 NCCL_IB_DISABLE=1
a182: [2023-04-17 17:13:43,897] [INFO] [launch.py:229:main] WORLD INFO DICT: &#123;'a182': [0, 1, 2, 3, 4, 5, 6, 7], 'a188': [0, 1, 2, 3, 4, 5, 6, 7]&#125;
a182: [2023-04-17 17:13:43,897] [INFO] [launch.py:235:main] nnodes=2, num_local_procs=8, node_rank=0
a182: [2023-04-17 17:13:43,897] [INFO] [launch.py:246:main] global_rank_mapping=defaultdict(<class 'list'>, &#123;'a182': [0, 1, 2, 3, 4, 5, 6, 7], 'a188': [8, 9, 10, 11, 12, 13, 14, 15]&#125;)
a182: [2023-04-17 17:13:43,897] [INFO] [launch.py:247:main] dist_world_size=16
a182: [2023-04-17 17:13:43,897] [INFO] [launch.py:249:main] Setting CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7
a188: [2023-04-17 17:13:43,896] [INFO] [launch.py:222:main] 1 NCCL_VERSION=2.16.5
a188: [2023-04-17 17:13:43,896] [INFO] [launch.py:222:main] 1 NCCL_SOCKET_IFNAME=eth0
a188: [2023-04-17 17:13:43,896] [INFO] [launch.py:222:main] 1 NCCL_IB_DISABLE=1
a188: [2023-04-17 17:13:43,896] [INFO] [launch.py:229:main] WORLD INFO DICT: &#123;'a182': [0, 1, 2, 3, 4, 5, 6, 7], 'a188': [0, 1, 2, 3, 4, 5, 6, 7]&#125;
a188: [2023-04-17 17:13:43,896] [INFO] [launch.py:235:main] nnodes=2, num_local_procs=8, node_rank=1
a188: [2023-04-17 17:13:43,896] [INFO] [launch.py:246:main] global_rank_mapping=defaultdict(<class 'list'>, &#123;'a182': [0, 1, 2, 3, 4, 5, 6, 7], 'a188': [8, 9, 10, 11, 12, 13, 14, 15]&#125;)
a188: [2023-04-17 17:13:43,896] [INFO] [launch.py:247:main] dist_world_size=16
a188: [2023-04-17 17:13:43,896] [INFO] [launch.py:249:main] Setting CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7
a182: [2023-04-17 17:13:54,469] [INFO] [comm.py:586:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
a182: 'HTTPSConnectionPool(host='huggingface.co', port=443): Max retries exceeded with url: /facebook/opt-1.3b/resolve/main/config.json (Caused by ConnectTimeoutError(<urllib3.connection.HTTPSConnection object at 0x7f214c1f8f70>, 'Connection to huggingface.co timed out. (connect timeout=10)'))' thrown while requesting HEAD https://huggingface.co/facebook/opt-1.3b/resolve/main/config.json
a182: [2023-04-17 17:17:31,616] [INFO] [partition_parameters.py:436:__exit__] finished initializing model with 1.42B parameters
a188: Found cached dataset parquet (/root/.cache/huggingface/datasets/Dahoas___parquet/default-b9d2c4937d617106/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec)
a188:
  0%|          | 0/2 [00:00<?, ?it/s]
 50%|█████     | 1/2 [00:00<00:00,  1.16it/s]
100%|██████████| 2/2 [00:00<00:00,  2.09it/s]
a182: Found cached dataset parquet (/root/.cache/huggingface/datasets/Dahoas___parquet/default-b9d2c4937d617106/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec)
a182:
  0%|          | 0/2 [00:00<?, ?it/s]Found cached dataset parquet (/root/.cache/huggingface/datasets/Dahoas___parquet/default-b9d2c4937d617106/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec)
a182:
  0%|          | 0/2 [00:00<?, ?it/s]
 50%|█████     | 1/2 [00:00<00:00,  3.89it/s]
 50%|█████     | 1/2 [00:01<00:01,  1.10s/it]
100%|██████████| 2/2 [00:01<00:00,  1.94it/s]
100%|██████████| 2/2 [00:00<00:00,  5.87it/s]
100%|██████████| 2/2 [00:00<00:00,  5.45it/s]
a182:
100%|██████████| 2/2 [00:01<00:00,  1.66it/s]
a182: Found cached dataset parquet (/root/.cache/huggingface/datasets/Dahoas___parquet/default-b9d2c4937d617106/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec)
a182:
  0%|          | 0/2 [00:00<?, ?it/s]
100%|██████████| 2/2 [00:00<00:00, 443.23it/s]
a182: Found cached dataset parquet (/root/.cache/huggingface/datasets/Dahoas___parquet/default-b9d2c4937d617106/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec)
a182:
  0%|          | 0/2 [00:00<?, ?it/s]
100%|██████████| 2/2 [00:00<00:00, 491.34it/s]
a182: Found cached dataset parquet (/root/.cache/huggingface/datasets/Dahoas___parquet/default-b9d2c4937d617106/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec)
a182:
  0%|          | 0/2 [00:00<?, ?it/s]
100%|██████████| 2/2 [00:00<00:00, 511.10it/s]
a182: Found cached dataset parquet (/root/.cache/huggingface/datasets/Dahoas___parquet/default-b9d2c4937d617106/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec)
a182:
  0%|          | 0/2 [00:00<?, ?it/s]
100%|██████████| 2/2 [00:00<00:00, 476.49it/s]
a182: Found cached dataset parquet (/root/.cache/huggingface/datasets/Dahoas___parquet/default-b9d2c4937d617106/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec)
a182:
  0%|          | 0/2 [00:00<?, ?it/s]
100%|██████████| 2/2 [00:00<00:00, 398.15it/s]
a182: Found cached dataset parquet (/root/.cache/huggingface/datasets/Dahoas___parquet/default-b9d2c4937d617106/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec)
a182:
  0%|          | 0/2 [00:00<?, ?it/s]
100%|██████████| 2/2 [00:00<00:00, 328.45it/s]
a188: Found cached dataset parquet (/root/.cache/huggingface/datasets/Dahoas___parquet/default-b9d2c4937d617106/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec)
a188:
  0%|          | 0/2 [00:00<?, ?it/s]
100%|██████████| 2/2 [00:00<00:00, 408.01it/s]
a188: Found cached dataset parquet (/root/.cache/huggingface/datasets/Dahoas___parquet/default-b9d2c4937d617106/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec)
a188:
  0%|          | 0/2 [00:00<?, ?it/s]
100%|██████████| 2/2 [00:00<00:00, 438.39it/s]
a188: Found cached dataset parquet (/root/.cache/huggingface/datasets/Dahoas___parquet/default-b9d2c4937d617106/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec)
a188:
  0%|          | 0/2 [00:00<?, ?it/s]
100%|██████████| 2/2 [00:00<00:00, 476.30it/s]
a188: Found cached dataset parquet (/root/.cache/huggingface/datasets/Dahoas___parquet/default-b9d2c4937d617106/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec)
a188:
  0%|          | 0/2 [00:00<?, ?it/s]
100%|██████████| 2/2 [00:00<00:00, 477.85it/s]
a188: Found cached dataset parquet (/root/.cache/huggingface/datasets/Dahoas___parquet/default-b9d2c4937d617106/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec)
a188:
  0%|          | 0/2 [00:00<?, ?it/s]
100%|██████████| 2/2 [00:00<00:00, 315.48it/s]
a188: Found cached dataset parquet (/root/.cache/huggingface/datasets/Dahoas___parquet/default-b9d2c4937d617106/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec)
a188:
  0%|          | 0/2 [00:00<?, ?it/s]
100%|██████████| 2/2 [00:00<00:00, 450.35it/s]
a188: Traceback (most recent call last):
a188:   File "main.py", line 339, in <module>
a188:     main()
a188:   File "main.py", line 218, in main
a188:     train_dataset, eval_dataset = create_prompt_dataset(
a188:   File "/data/repos/DeepSpeedExamples/applications/DeepSpeed-Chat/training/utils/data/data_utils.py", line 279, in create_prompt_dataset
a188:     train_dataset, eval_dataset = create_dataset(
a188:   File "/data/repos/DeepSpeedExamples/applications/DeepSpeed-Chat/training/utils/data/data_utils.py", line 212, in create_dataset
a188:     raw_dataset = get_raw_dataset(dataset_name, output_path, seed, local_rank)
a188:   File "/data/repos/DeepSpeedExamples/applications/DeepSpeed-Chat/training/utils/data/data_utils.py", line 21, in get_raw_dataset
a188:     return raw_datasets.DahoasRmstaticDataset(output_path, seed,
a188:   File "/data/repos/DeepSpeedExamples/applications/DeepSpeed-Chat/training/utils/data/raw_datasets.py", line 52, in __init__
a188:     self.raw_datasets = load_dataset("Dahoas/rm-static")
a188:   File "/usr/local/lib/python3.8/dist-packages/datasets/load.py", line 1767, in load_dataset
a188:     builder_instance = load_dataset_builder(
a188:   File "/usr/local/lib/python3.8/dist-packages/datasets/load.py", line 1498, in load_dataset_builder
a188:     dataset_module = dataset_module_factory(
a188:   File "/usr/local/lib/python3.8/dist-packages/datasets/load.py", line 1215, in dataset_module_factory
a188:     raise e1 from None
a188:   File "/usr/local/lib/python3.8/dist-packages/datasets/load.py", line 1192, in dataset_module_factory
a188:     return HubDatasetModuleFactoryWithoutScript(
a188:   File "/usr/local/lib/python3.8/dist-packages/datasets/load.py", line 825, in get_module
a188:     dataset_readme_path = cached_path(
a188:   File "/usr/local/lib/python3.8/dist-packages/datasets/utils/file_utils.py", line 183, in cached_path
a188:     output_path = get_from_cache(
a188:   File "/usr/local/lib/python3.8/dist-packages/datasets/utils/file_utils.py", line 566, in get_from_cache
a188:     raise ConnectionError(f"Couldn't reach &#123;url&#125; (&#123;repr(head_error)&#125;)")
a188: ConnectionError: Couldn't reach https://huggingface.co/datasets/Dahoas/rm-static/resolve/main/README.md (ReadTimeout(ReadTimeoutError("HTTPSConnectionPool(host='huggingface.co', port=443): Read timed out. (read timeout=100)")))
a188: [2023-04-17 18:19:46,372] [INFO] [launch.py:428:sigkill_handler] Killing subprocess 9324
a188: [2023-04-17 18:19:46,591] [INFO] [launch.py:428:sigkill_handler] Killing subprocess 9325
a188: [2023-04-17 18:19:46,806] [INFO] [launch.py:428:sigkill_handler] Killing subprocess 9326
a188: [2023-04-17 18:19:46,980] [INFO] [launch.py:428:sigkill_handler] Killing subprocess 9327
a188: [2023-04-17 18:19:47,195] [INFO] [launch.py:428:sigkill_handler] Killing subprocess 9328
a188: [2023-04-17 18:19:47,369] [INFO] [launch.py:428:sigkill_handler] Killing subprocess 9329
a188: [2023-04-17 18:19:47,542] [INFO] [launch.py:428:sigkill_handler] Killing subprocess 9330
a188: [2023-04-17 18:19:47,543] [INFO] [launch.py:428:sigkill_handler] Killing subprocess 9332
a188: [2023-04-17 18:19:47,917] [ERROR] [launch.py:434:sigkill_handler] ['/usr/bin/python', '-u', 'main.py', '--local_rank=7', '--data_path', 'Dahoas/rm-static', 'Dahoas/full-hh-rlhf', 'Dahoas/synthetic-instruct-gptj-pairwise', 'yitingxie/rlhf-reward-datasets', 'openai/webgpt_comparisons', 'stanfordnlp/SHP', '--data_split', '2,4,4', '--model_name_or_path', 'facebook/opt-1.3b', '--per_device_train_batch_size', '4', '--per_device_eval_batch_size', '4', '--max_seq_len', '512', '--learning_rate', '1e-4', '--weight_decay', '0.1', '--num_train_epochs', '2', '--gradient_accumulation_steps', '1', '--lr_scheduler_type', 'cosine', '--num_warmup_steps', '0', '--seed', '1234', '--gradient_checkpointing', '--zero_stage', '3', '--lora_dim', '128', '--lora_module_name', 'decoder.layers.', '--deepspeed', '--output_dir', '/data/opt-1.3b-multi-node/actor-models/1.3b'] exits with return code = 1
pdsh@ecs-21075649-010: a188: ssh exited with exit code 1
pdsh@ecs-21075649-010: interrupt (one more within 1 sec to abort)
pdsh@ecs-21075649-010:  (^Z within 1 sec to cancel pending threads)
pdsh@ecs-21075649-010: a182: command in progress
sending SIGTERM to ssh a182
sending signal 15 to a182 [ssh] pid 5758
pdsh@ecs-21075649-010: interrupt, aborting.
pdsh@ecs-21075649-010: a188: ssh exited with exit code 1
  </code></pre>
</details>

<h2 id="性能对比测试"><a href="#性能对比测试" class="headerlink" title="性能对比测试"></a>性能对比测试</h2><h3 id="实验一"><a href="#实验一" class="headerlink" title="实验一"></a>实验一</h3><p>4x A100 8卡 A100-PCIE-40GB</p>
<p>【step1】</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">deepspeed --hostfile=<span class="variable">$HOSTFILE</span> main.py \</span><br><span class="line">   --data_path all_instruction_data_for_DeepSpeedChat \</span><br><span class="line">   --data_split 2,4,4 \</span><br><span class="line">   --model_name_or_path /data/models/bigscience_bloomz-7b1 \</span><br><span class="line">   --per_device_train_batch_size 4 \</span><br><span class="line">   --per_device_eval_batch_size 4 \</span><br><span class="line">   --max_seq_len 2048 \</span><br><span class="line">   --learning_rate 1e-4 \</span><br><span class="line">   --weight_decay 0.1 \</span><br><span class="line">   --num_train_epochs 2  \</span><br><span class="line">   --gradient_accumulation_steps 1 \</span><br><span class="line">   --lr_scheduler_type cosine \</span><br><span class="line">   --num_warmup_steps 0 \</span><br><span class="line">   --seed 1234 \</span><br><span class="line">   --gradient_checkpointing \</span><br><span class="line">   --zero_stage <span class="variable">$ZERO_STAGE</span> \</span><br><span class="line">   --lora_dim 128 \</span><br><span class="line">   --lora_module_name decoder.layers. \</span><br><span class="line">   --deepspeed \</span><br><span class="line">   --output_dir <span class="variable">$OUTPUT</span> \</span><br><span class="line">   &amp;&gt; <span class="variable">$OUTPUT</span>/training.log</span><br></pre></td></tr></table></figure>

<p>【step3】</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line">deepspeed --hostfile=<span class="variable">$HOSTFILE</span> --master_port 12346 main.py \</span><br><span class="line">   --data_path all_instruction_data_for_DeepSpeedChat \</span><br><span class="line">   --data_split 2,4,4 \</span><br><span class="line">   --actor_model_name_or_path <span class="variable">$ACTOR_MODEL_PATH</span> \</span><br><span class="line">   --critic_model_name_or_path <span class="variable">$CRITIC_MODEL_PATH</span> \</span><br><span class="line">   --num_padding_at_beginning 1 \</span><br><span class="line">   --per_device_train_batch_size 4 \</span><br><span class="line">   --per_device_mini_train_batch_size 4 \</span><br><span class="line">   --generation_batch_numbers 1 \</span><br><span class="line">   --ppo_epochs 1 \</span><br><span class="line">   --max_answer_seq_len 1024 \</span><br><span class="line">   --max_prompt_seq_len 1024 \</span><br><span class="line">   --actor_learning_rate <span class="variable">$&#123;Actor_Lr&#125;</span> \</span><br><span class="line">   --critic_learning_rate <span class="variable">$&#123;Critic_Lr&#125;</span> \</span><br><span class="line">   --actor_weight_decay 0.1 \</span><br><span class="line">   --critic_weight_decay 0.1 \</span><br><span class="line">   --num_train_epochs 1 \</span><br><span class="line">   --lr_scheduler_type cosine \</span><br><span class="line">   --gradient_accumulation_steps 1 \</span><br><span class="line">   --num_warmup_steps 100 \</span><br><span class="line">   --deepspeed --seed 1234 \</span><br><span class="line">   --enable_hybrid_engine \</span><br><span class="line">   --inference_tp_size 8 \</span><br><span class="line">   --tp_gather_partition_size 4 \</span><br><span class="line">   --actor_zero_stage <span class="variable">$ACTOR_ZERO_STAGE</span> \</span><br><span class="line">   --critic_zero_stage <span class="variable">$CRITIC_ZERO_STAGE</span> \</span><br><span class="line">   --actor_gradient_checkpointing \</span><br><span class="line">   --actor_lora_dim 128 \</span><br><span class="line">   --actor_lora_module_name decoder.layers. \</span><br><span class="line">   --output_dir <span class="variable">$OUTPUT</span> \</span><br><span class="line">    &amp;&gt; <span class="variable">$OUTPUT</span>/training.log</span><br></pre></td></tr></table></figure>

<p>【training.log】</p>
<details>
  <summary>点击展开</summary>
  <pre><code>
a182: ***** Running training *****
a182: ***** Evaluating perplexity, Epoch 0/2 *****
a182: ppl: 1615.3291015625
a182: Beginning of Epoch 1/2, Total Micro Batches 844
a182: [2023-04-21 15:32:05,774] [INFO] [loss_scaler.py:188:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, but hysteresis is 2. Reducing hysteresis to 1
a182: [2023-04-21 15:32:47,044] [INFO] [loss_scaler.py:181:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, reducing to 32768
a182: [2023-04-21 15:33:29,700] [INFO] [loss_scaler.py:181:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, reducing to 16384
a182: [2023-04-21 15:34:11,199] [INFO] [loss_scaler.py:181:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 16384, reducing to 8192
a182: [2023-04-21 15:34:53,648] [INFO] [loss_scaler.py:181:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 8192, reducing to 4096
a182: [2023-04-21 15:35:37,038] [INFO] [loss_scaler.py:181:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 4096, reducing to 2048
a182: [2023-04-21 15:36:19,724] [INFO] [loss_scaler.py:181:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 2048, reducing to 1024
a182: [2023-04-21 15:37:02,075] [INFO] [loss_scaler.py:181:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 1024, reducing to 512
a182: [2023-04-21 15:37:44,410] [INFO] [loss_scaler.py:181:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 512, reducing to 256
a182: [2023-04-21 15:38:26,336] [INFO] [loss_scaler.py:181:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 256, reducing to 128
a182: [2023-04-21 15:38:26,337] [INFO] [logging.py:96:log_dist] [Rank 0] step=10, skipped=10, lr=[0.0001, 0.0001], mom=[(0.9, 0.95), (0.9, 0.95)]
a182: [2023-04-21 15:38:26,338] [INFO] [timer.py:199:stop] epoch=0/micro_step=10/global_step=10, RunningAvgSamplesPerSec=3.018237674524317, CurrSamplesPerSec=3.0533144699669323, MemAllocated=8.18GB, MaxMemAllocated=27.21GB
a182: [2023-04-21 15:39:09,043] [WARNING] [stage3.py:1787:step] 17 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
a182: [2023-04-21 15:39:50,772] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
a182: [2023-04-21 15:40:32,341] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
a182: [2023-04-21 15:41:14,049] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
a182: [2023-04-21 15:41:55,981] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
a182: [2023-04-21 15:42:38,949] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
a182: [2023-04-21 15:44:05,257] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
a182: [2023-04-21 15:44:47,426] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
a182: [2023-04-21 15:45:29,589] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
a182: [2023-04-21 15:45:29,590] [INFO] [logging.py:96:log_dist] [Rank 0] step=20, skipped=10, lr=[9.999134070902207e-05, 9.999134070902207e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
a182: [2023-04-21 15:45:29,591] [INFO] [timer.py:199:stop] epoch=0/micro_step=20/global_step=20, RunningAvgSamplesPerSec=3.021699438930297, CurrSamplesPerSec=3.0359636074270875, MemAllocated=8.18GB, MaxMemAllocated=27.21GB
a182: [2023-04-21 15:46:10,884] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
a182: [2023-04-21 15:46:53,122] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
a182: [2023-04-21 15:47:34,534] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
a182: [2023-04-21 15:48:16,075] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
a182: [2023-04-21 15:48:58,285] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
a182: [2023-04-21 15:49:40,420] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
a182: [2023-04-21 15:50:23,103] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
a182: [2023-04-21 15:51:06,003] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
a182: [2023-04-21 15:51:47,326] [WARNING] [stage3.py:1787:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
a182: [2023-04-21 15:52:29,013] [WARNING] [stage3.py:1787:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
a182: [2023-04-21 15:52:29,014] [INFO] [logging.py:96:log_dist] [Rank 0] step=30, skipped=10, lr=[9.996536583542105e-05, 9.996536583542105e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
a182: [2023-04-21 15:52:29,015] [INFO] [timer.py:199:stop] epoch=0/micro_step=30/global_step=30, RunningAvgSamplesPerSec=3.0324580151109792, CurrSamplesPerSec=3.0706646549224987, MemAllocated=8.18GB, MaxMemAllocated=27.21GB
  </code></pre>
</details>

<p>【GPU Loading】</p>
<img src="/images/nccl/fig12.png" class="" width="400" height="444" title="nvidia-smi">

<p>【内存占用】</p>
<img src="/images/nccl/fig13.png" class="" width="960" height="540" title="htop">

<h3 id="实验二"><a href="#实验二" class="headerlink" title="实验二"></a>实验二</h3><p>6x A100 8卡 A100-PCIE-40GB<br>训练参数与实验1完全相同，只是增加了两台机器</p>
<p>【结论】：还是会有显存高压的警告，不过没有连续出现。担心step3 还是会爆掉。先跑完step1，后面可以单独跑step2,3</p>
<p>【training.log】</p>
<details>
  <summary>点击展开</summary>
  <pre><code>
a182: Time to load utils op: 0.0005230903625488281 seconds
a182: ***** Running training *****
a182: ***** Evaluating perplexity, Epoch 0/2 *****
a182: ppl: 1615.9046630859375
a182: Beginning of Epoch 1/2, Total Micro Batches 563
a182: [2023-04-22 00:20:37,845] [INFO] [loss_scaler.py:188:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, but hysteresis is 2. Reducing hysteresis to 1
a182: [2023-04-22 00:21:23,520] [INFO] [loss_scaler.py:181:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, reducing to 32768
a182: [2023-04-22 00:22:08,163] [INFO] [loss_scaler.py:181:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, reducing to 16384
a182: [2023-04-22 00:22:54,065] [INFO] [loss_scaler.py:181:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 16384, reducing to 8192
a182: [2023-04-22 00:23:40,058] [INFO] [loss_scaler.py:181:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 8192, reducing to 4096
a182: [2023-04-22 00:24:25,473] [INFO] [loss_scaler.py:181:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 4096, reducing to 2048
a182: [2023-04-22 00:25:12,089] [INFO] [loss_scaler.py:181:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 2048, reducing to 1024
a182: [2023-04-22 00:25:58,308] [INFO] [loss_scaler.py:181:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 1024, reducing to 512
a182: [2023-04-22 00:26:43,708] [INFO] [loss_scaler.py:181:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 512, reducing to 256
a182: [2023-04-22 00:27:30,709] [INFO] [loss_scaler.py:181:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 256, reducing to 128
a182: [2023-04-22 00:27:30,711] [INFO] [logging.py:96:log_dist] [Rank 0] step=10, skipped=10, lr=[0.0001, 0.0001], mom=[(0.9, 0.95), (0.9, 0.95)]
a182: [2023-04-22 00:27:30,712] [INFO] [timer.py:199:stop] epoch=0/micro_step=10/global_step=10, RunningAvgSamplesPerSec=4.183477766906136, CurrSamplesPerSec=4.0852692074479595, MemAllocated=7.04GB, MaxMemAllocated=26.07GB
a182: [2023-04-22 00:28:17,487] [WARNING] [stage3.py:1787:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
a182: [2023-04-22 00:35:01,687] [INFO] [logging.py:96:log_dist] [Rank 0] step=20, skipped=10, lr=[9.99805403600595e-05, 9.99805403600595e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
a182: [2023-04-22 00:35:01,688] [INFO] [timer.py:199:stop] epoch=0/micro_step=20/global_step=20, RunningAvgSamplesPerSec=4.224439829942313, CurrSamplesPerSec=4.238250039396783, MemAllocated=7.04GB, MaxMemAllocated=26.07GB
a182: [2023-04-22 00:42:17,326] [INFO] [logging.py:96:log_dist] [Rank 0] step=30, skipped=10, lr=[9.99221765873415e-05, 9.99221765873415e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
a182: [2023-04-22 00:42:17,327] [INFO] [timer.py:199:stop] epoch=0/micro_step=30/global_step=30, RunningAvgSamplesPerSec=4.288112235651409, CurrSamplesPerSec=4.507034898076392, MemAllocated=7.04GB, MaxMemAllocated=26.07GB
a182: [2023-04-22 00:49:22,478] [INFO] [logging.py:96:log_dist] [Rank 0] step=40, skipped=10, lr=[9.982495411136606e-05, 9.982495411136606e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
a182: [2023-04-22 00:49:22,479] [INFO] [timer.py:199:stop] epoch=0/micro_step=40/global_step=40, RunningAvgSamplesPerSec=4.345926499891864, CurrSamplesPerSec=4.512978428671682, MemAllocated=7.04GB, MaxMemAllocated=26.07GB
  </code></pre>
</details>

<p>【GPU Loading】</p>
<img src="/images/nccl/fig14.png" class="" width="400" height="444" title="nvidia-smi">

<p>【内存占用】</p>
<img src="/images/nccl/fig15.png" class="" width="960" height="540" title="htop">
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">文章作者: </span><span class="post-copyright-info"><a href="https://three-body.com.cn">waterdropw</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">文章链接: </span><span class="post-copyright-info"><a href="https://three-body.com.cn/2023/09/28/NCCL-Debug/">https://three-body.com.cn/2023/09/28/NCCL-Debug/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="https://three-body.com.cn" target="_blank">waterdropw 的博客</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"></div><div class="post_share"><div class="social-share" data-image="/images/waterdrop.jpeg" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><div class="next-post pull-full"><a href="/2023/09/28/LLaMA/" title="论文摘要-LLaMA"><div class="cover" style="background: var(--default-bg-color)"></div><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">论文摘要-LLaMA</div></div></a></div></nav><hr class="custom-hr"/><div id="post-comment"><div class="comment-head"><div class="comment-headline"><i class="fas fa-comments fa-fw"></i><span> 评论</span></div></div><div class="comment-wrap"><div><div id="gitalk-container"></div></div></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="is-center"><div class="avatar-img"><img src="/images/waterdrop.jpeg" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info__name">waterdropw</div><div class="author-info__description">技术博客，杂记，随笔</div></div><div class="card-info-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">9</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">0</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">0</div></a></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/waterdropw"><i class="fab fa-github"></i><span>Follow Me</span></a><div class="card-info-social-icons is-center"><a class="social-icon" href="https://github.com/waterdropw" target="_blank" title="Github"><i class="fab fa-github" style="color: #24292e;"></i></a><a class="social-icon" href="mailto:xiaobin.wee@gmail.com" target="_blank" title="Email"><i class="fas fa-envelope" style="color: #4a7dbe;"></i></a></div></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>公告</span></div><div class="announcement_content">欢迎来到三体的世界，我是水滴💧！</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%B5%8B%E8%AF%95%E4%B8%80"><span class="toc-number">1.</span> <span class="toc-text">测试一</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%8E%AF%E5%A2%83%E9%85%8D%E7%BD%AE"><span class="toc-number">1.1.</span> <span class="toc-text">环境配置</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#log-%E5%88%86%E6%9E%90"><span class="toc-number">1.2.</span> <span class="toc-text">log 分析</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%B5%8B%E8%AF%95%E4%BA%8C"><span class="toc-number">2.</span> <span class="toc-text">测试二</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%8E%AF%E5%A2%83%E9%85%8D%E7%BD%AE-1"><span class="toc-number">2.1.</span> <span class="toc-text">环境配置</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#log-%E5%88%86%E6%9E%90-1"><span class="toc-number">2.2.</span> <span class="toc-text">log 分析</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%B5%8B%E8%AF%95%E4%B8%89"><span class="toc-number">3.</span> <span class="toc-text">测试三</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%8E%AF%E5%A2%83%E9%85%8D%E7%BD%AE-2"><span class="toc-number">3.1.</span> <span class="toc-text">环境配置</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%B5%8B%E8%AF%95%E5%9B%9B"><span class="toc-number">4.</span> <span class="toc-text">测试四</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%8E%AF%E5%A2%83%E9%85%8D%E7%BD%AE-3"><span class="toc-number">4.1.</span> <span class="toc-text">环境配置</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%9C%80%E7%BB%88%E8%A7%A3%E5%86%B3%E6%96%B9%E6%A1%88"><span class="toc-number">5.</span> <span class="toc-text">最终解决方案</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%80%A7%E8%83%BD%E5%AF%B9%E6%AF%94%E6%B5%8B%E8%AF%95"><span class="toc-number">6.</span> <span class="toc-text">性能对比测试</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%AE%9E%E9%AA%8C%E4%B8%80"><span class="toc-number">6.1.</span> <span class="toc-text">实验一</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%AE%9E%E9%AA%8C%E4%BA%8C"><span class="toc-number">6.2.</span> <span class="toc-text">实验二</span></a></li></ol></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2023/09/28/NCCL-Debug/" title="多机多卡训练：NCCL Debug">多机多卡训练：NCCL Debug</a><time datetime="2023-09-28T08:05:10.000Z" title="发表于 2023-09-28 16:05:10">2023-09-28</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2023/09/28/LLaMA/" title="论文摘要-LLaMA">论文摘要-LLaMA</a><time datetime="2023-09-28T06:48:11.000Z" title="发表于 2023-09-28 14:48:11">2023-09-28</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2020/12/25/Compiler-Bug-for-C-Funcs-Override/" title="C++ 函数实现的异常覆盖">C++ 函数实现的异常覆盖</a><time datetime="2020-12-25T10:09:59.000Z" title="发表于 2020-12-25 18:09:59">2020-12-25</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2020/11/18/Android-Camera-HAL-Treble/" title="Android Camera HAL 新架构">Android Camera HAL 新架构</a><time datetime="2020-11-18T09:40:24.000Z" title="发表于 2020-11-18 17:40:24">2020-11-18</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2020/11/12/Android-External-USB-Cameras/" title="Android 外接 USB 摄像头">Android 外接 USB 摄像头</a><time datetime="2020-11-12T09:42:17.000Z" title="发表于 2020-11-12 17:42:17">2020-11-12</time></div></div></div></div></div></div></main><footer id="footer"><div id="footer-wrap"><div class="copyright">&copy;2020 - 2024 By waterdropw</div><div class="framework-info"><span>框架 </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>主题 </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside_config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><button id="chat-btn" type="button" title="聊天"><i class="fas fa-sms"></i></button><a id="to_comment" href="#post-comment" title="直达评论"><i class="fas fa-comments"></i></a><button id="go-up" type="button" title="回到顶部"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox/fancybox.umd.min.js"></script><script>function panguFn () {
  if (typeof pangu === 'object') pangu.autoSpacingPage()
  else {
    getScript('https://cdn.jsdelivr.net/npm/pangu/dist/browser/pangu.min.js')
      .then(() => {
        pangu.autoSpacingPage()
      })
  }
}

function panguInit () {
  if (false){
    GLOBAL_CONFIG_SITE.isPost && panguFn()
  } else {
    panguFn()
  }
}

document.addEventListener('DOMContentLoaded', panguInit)</script><div class="js-pjax"><script>if (!window.MathJax) {
  window.MathJax = {
    tex: {
      inlineMath: [['$', '$'], ['\\(', '\\)']],
      tags: 'ams'
    },
    chtml: {
      scale: 1.1
    },
    options: {
      renderActions: {
        findScript: [10, doc => {
          for (const node of document.querySelectorAll('script[type^="math/tex"]')) {
            const display = !!node.type.match(/; *mode=display/)
            const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display)
            const text = document.createTextNode('')
            node.parentNode.replaceChild(text, node)
            math.start = {node: text, delim: '', n: 0}
            math.end = {node: text, delim: '', n: 0}
            doc.math.push(math)
          }
        }, '']
      }
    }
  }
  
  const script = document.createElement('script')
  script.src = 'https://cdn.jsdelivr.net/npm/mathjax/es5/tex-mml-chtml.min.js'
  script.id = 'MathJax-script'
  script.async = true
  document.head.appendChild(script)
} else {
  MathJax.startup.document.state(0)
  MathJax.texReset()
  MathJax.typesetPromise()
}</script><script>function loadGitalk () {
  function initGitalk () {
    var gitalk = new Gitalk(Object.assign({
      clientID: '021e1b337f2aa7e7f74b',
      clientSecret: 'dcb3bc17f4024eda15077a2f4373ee03f7a4d986',
      repo: 'waterdropw.github.io',
      owner: 'waterdropw',
      admin: ['waterdropw'],
      id: '1da5ce8fe847cf6cebad444e57c24667',
      updateCountCallback: commentCount
    },null))

    gitalk.render('gitalk-container')
  }

  if (typeof Gitalk === 'function') initGitalk()
  else {
    getCSS('https://cdn.jsdelivr.net/npm/gitalk/dist/gitalk.min.css')
    getScript('https://cdn.jsdelivr.net/npm/gitalk/dist/gitalk.min.js').then(initGitalk)
  }
}

function commentCount(n){
  let isCommentCount = document.querySelector('#post-meta .gitalk-comment-count')
  if (isCommentCount) {
    isCommentCount.textContent= n
  }
}

if ('Gitalk' === 'Gitalk' || !true) {
  if (true) btf.loadComment(document.getElementById('gitalk-container'), loadGitalk)
  else loadGitalk()
} else {
  function loadOtherComment () {
    loadGitalk()
  }
}</script></div><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/dist/activate-power-mode.min.js"></script><script>POWERMODE.colorful = true;
POWERMODE.shake = true;
POWERMODE.mobile = false;
document.body.addEventListener('input', POWERMODE);
</script><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div></body></html>